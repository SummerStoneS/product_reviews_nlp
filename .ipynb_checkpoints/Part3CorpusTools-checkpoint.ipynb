{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c403cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413083c",
   "metadata": {},
   "source": [
    "After人工整理场景人群首购复购精神认同相关的聚类段落\n",
    "需要先去重后再做进一步的精准修剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cdde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重的优先级，如果在negative和no_keyword_negative里都有，那么最后出现在no_keyword_negative里\n",
    "senti_rank = {'no_keyword_negative':5, \n",
    "              'no_keyword_positive':4,\n",
    "              'negative':2,\n",
    "              'positive':1,\n",
    "              'no_keyword_neutral':6,\n",
    "              'neutral':3,\n",
    "              'no_keyword':7\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b6a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_row_paragraphs_to_columns_with_exist_rank(rebuy, index_columns_names, expand_column_name):\n",
    "    \"\"\"\n",
    "        rebuy: 聚类段落数据\n",
    "        index_columns_names: 标识段落属性的index, 比如id, cluster_size, center_sentences\n",
    "        expand_column_name: 段落列，要把,拼接的段落变成一行一句话\n",
    "    \"\"\"\n",
    "    index_columns = np.array(rebuy[index_columns_names])\n",
    "    paragraphs = np.array(rebuy[expand_column_name])\n",
    "    \n",
    "    row_index_groups = []\n",
    "    new_df = pd.DataFrame()\n",
    "    for row_index, paragraph in zip(index_columns, paragraphs):\n",
    "        row_index_groups.append(row_index)\n",
    "        sentence_list = paragraph.split(',')\n",
    "        paragraph_len = len(sentence_list)\n",
    "        row_index = list(row_index)\n",
    "        a = pd.concat([pd.DataFrame([row_index] *paragraph_len, columns = index_columns_names), \n",
    "                       pd.Series(sentence_list, name=expand_column_name)], axis=1)\n",
    "        new_df = pd.concat([new_df, a])\n",
    "    new_df = new_df.set_index(index_columns_names)\n",
    "    return new_df, row_index_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8d6f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_data(data, *args):\n",
    "    return data.loc[pd.IndexSlice[args], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73b5f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_to_flat_cluster(new_df, row_index_groups):\n",
    "    final_data = []\n",
    "    for row_index in row_index_groups:\n",
    "        try:\n",
    "            row_index = list(row_index)\n",
    "            para_list = get_index_data(new_df, *row_index)\n",
    "            para = \",\".join(para_list[column_to_clean].to_list())\n",
    "            para_len = len(para_list)\n",
    "            row_content = row_index + [para_len, para]\n",
    "            final_data.append(row_content)\n",
    "        except:\n",
    "            print(\"这个类没了：\")\n",
    "            print(row_index)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "70b556f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(df):\n",
    "    return pd.DataFrame({\"段落_去重\": itertools.chain(*df[\"段落_更新\"].str.split(\",\"))})\n",
    "# rebuy.groupby(['tag_1', 'tag_2', '评论数', 'NLP']).apply(expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1cd22",
   "metadata": {},
   "source": [
    "### 精神认同整理 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "88553bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet_name in ['首购复购', '人群', \"适用场景\", \"精神认同\"]:\n",
    "\n",
    "    rebuy = pd.read_excel(r\"场景人群代言复购语料库\\场景人群复购精神认同语料库4.xlsx\", sheet_name = sheet_name)\n",
    "    rebuy = rebuy[rebuy[\"tag_1\"].notnull()]\n",
    "    rebuy['tag_2'].fillna('TBD', inplace=True)\n",
    "    rebuy.drop('段落', axis=1, inplace=True)\n",
    "    \n",
    "    rebuy = rebuy.reset_index()   # 记住原来的index，为了后面去完重按照原始的顺序输出\n",
    "    index_columns_names = list(rebuy.columns[:5])\n",
    "    column_to_clean = '段落_更新'\n",
    "    \n",
    "    rebuy = rebuy.sort_values(by='NLP', key=lambda x: x.replace(senti_rank))  # 根据自定义顺序去重\n",
    "    \n",
    "    new_df, row_index_groups = expand_row_paragraphs_to_columns_with_exist_rank(rebuy,\n",
    "                                                                                index_columns_names, \n",
    "                                                                                expand_column_name=column_to_clean)\n",
    "    \n",
    "    new_df = new_df.drop_duplicates(subset = [column_to_clean], keep='first')\n",
    "    final_data = revert_to_flat_cluster(new_df, row_index_groups)\n",
    "    index_columns_names.extend(['评论数_去重','去重段落'])\n",
    "    result = pd.DataFrame(final_data, columns=index_columns_names).sort_values(by='index')\n",
    "    \n",
    "    result.to_excel(f\"{sheet_name}.xlsx\",index=False)\n",
    "    \n",
    "#整理生成的excel贴到原始的场景人群复购精神认同语料库4.xlsx里"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb5165",
   "metadata": {},
   "source": [
    "### 把所有聚类结果合并去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c289bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_column(df):\n",
    "    return pd.DataFrame({\"去重段落\": itertools.chain(*df['cluster_sentences'].str.split(','))})\n",
    "\n",
    "def fold_back(df):\n",
    "    return \",\".join(list(df.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53f9ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接所有情感类别的聚类结果\n",
    "file_list = glob.glob(r\"model_result\\cluster\\classify_sample_1_*_ascending_*.xlsx\")\n",
    "combine_cluster_result = pd.DataFrame()\n",
    "for file_name in file_list:\n",
    "    sentiment_type = re.match(r'.+classify_sample_1_(.+)_cos_kmeans', file_name).group(1)\n",
    "    if sentiment_type == 'no_keyword':   # 最早keyword没有按照三种情感分别聚类，不分情感的现在不需要了（已检查过没有什么价值）\n",
    "        continue\n",
    "    data = pd.read_excel(file_name)\n",
    "    data = pd.concat([pd.Series([sentiment_type]*len(data), name='sentiment'), data], axis=1)\n",
    "    combine_cluster_result = pd.concat([combine_cluster_result, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5aabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照自定义优先级对段落句子去重\n",
    "grp_index = ['sentiment', 'cluster_id', 'cluster_size','center_sentence']\n",
    "column_to_clean = 'cluster_sentences'\n",
    "\n",
    "combine_cluster_result = combine_cluster_result.sort_values(by='sentiment', key=lambda x: x.replace(senti_rank))\n",
    "new_df, row_index_groups = expand_row_paragraphs_to_columns_with_exist_rank(combine_cluster_result,\n",
    "                                                                            grp_index, \n",
    "                                                                            expand_column_name=column_to_clean)\n",
    "\n",
    "new_df = new_df.drop_duplicates(subset = [column_to_clean], keep='first')\n",
    "final_data = revert_to_flat_cluster(new_df, row_index_groups)\n",
    "grp_index.extend([\"去重段落数\", \"去重段落\"])\n",
    "combine_cluster_no_dup = pd.DataFrame(final_data, columns=grp_index)\n",
    "combine_cluster_no_dup.to_excel(\"step_data/cluster_result_combine_all_sentiments_no_dup_update.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2c1eb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二\n",
    "grp_index = ['sentiment', 'cluster_id', 'cluster_size','center_sentence']\n",
    "# 把每个类里面由,拼接的多个句子扩展变成一列，每行一句话\n",
    "combine_cluster_result = combine_cluster_result.groupby(grp_index).apply(expand_column).reset_index()\n",
    "\n",
    "#按照senti_rank自定义优先级去重\n",
    "combine_cluster_result = combine_cluster_result.sort_values(by=['sentiment','level_4'], key=lambda x: x.replace(senti_rank))\n",
    "# combine_cluster_result = combine_cluster_result.sort_values(by=['sentiment'], key=lambda x: x.replace(senti_rank)) #坑版\n",
    "\n",
    "remove_duplicate = combine_cluster_result.drop_duplicates(subset=['去重段落'], keep='first')\n",
    "\n",
    "# 把每个类的句子再逗号拼接回去\n",
    "columns_name = grp_index + [\"去重段落数\", \"去重段落\"]\n",
    "combine_cluster_no_dup = remove_duplicate.groupby(grp_index).agg({'去重段落':['count', fold_back]}).reset_index()\n",
    "combine_cluster_no_dup.columns = columns_name\n",
    "combine_cluster_no_dup = combine_cluster_no_dup.sort_values(by='sentiment', key=lambda x: x.replace(senti_rank))\n",
    "combine_cluster_no_dup.to_excel(\"step_data/cluster_result_combine_all_sentiments_no_dup.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3311e32",
   "metadata": {},
   "source": [
    "### 如果是已经拿出来的人群，适用场景，精神认同，首购复购，则先打上tag_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55ae3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_cluster_no_dup = pd.read_excel(\"step_data/cluster_result_combine_all_sentiments_no_dup_update.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cfccb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name='首购复购'\n",
    "tagged_cluster = pd.DataFrame()\n",
    "for sheet_name in ['首购复购', '人群', \"适用场景\", \"精神认同\"]:\n",
    "    data = pd.read_excel(r\"场景人群代言复购语料库\\场景人群复购精神认同语料库_keysentence.xlsx\", sheet_name = sheet_name)\n",
    "    data = data[data[\"tag_1\"].notnull()]\n",
    "    data['中心语句'] = data['中心语句'].fillna(pd.Series(data[\"段落\"].str.split(',')[0]))\n",
    "    tagged_cluster = pd.concat([tagged_cluster, data[['中心语句','tag_1','NLP']]])\n",
    "    \n",
    "tagged_cluster.columns = ['center_sentence', 'tag_1', 'sentiment']\n",
    "tagged_cluster = tagged_cluster.drop_duplicates()\n",
    "cluster_tag = pd.merge(combine_cluster_no_dup, tagged_cluster, on=['sentiment','center_sentence'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40325",
   "metadata": {},
   "source": [
    "#### 其余的根据中心语句的关键字和关键词词表打标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1a8f9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict(r\"model_input/keywords/add_keywords_for_jieba.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23377a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取关键词\n",
    "keyword_tag=pd.read_excel(r'model_input\\keywords\\keywords_sentimental_words.xlsx')\n",
    "keyword_tag=keyword_tag.loc[:,['关键字','一级','二级','情感倾向']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2ede199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(s):\n",
    "    for word in s:\n",
    "        if word in keyword_tag['关键字'].to_list():\n",
    "            return word\n",
    "    return None\n",
    "cluster_tag['中心语断句'] = cluster_tag['center_sentence'].apply(lambda x: list(jieba.cut(x, cut_all=True)))\n",
    "cluster_tag['关键字'] = cluster_tag['中心语断句'].apply(find_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "264908e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_tag = pd.merge(cluster_tag, keyword_tag, on='关键字',how='left')\n",
    "cluster_tag['tag_1'] = cluster_tag['tag_1'].fillna(cluster_tag['一级'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "474cb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_tag.to_excel(\"step_data/cluster_result_combine_all_sentiments_no_dup_tag4.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff291e73",
   "metadata": {},
   "source": [
    "### ---------------end\n",
    "到此结束"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
