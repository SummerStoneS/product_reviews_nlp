{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e1c79b",
   "metadata": {},
   "source": [
    "### 先计算关键词词频，选词频最高的5个词，设置阈值，如果排名第一的词频小于0.1，这个类直接就是5\n",
    "### 如果关键词在人群、场景、复购、精神认同里，则打上psbe标签\n",
    "### 前5个关键词每个词去找一级、二级属性；判断一级，二级种类数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',10000)\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5101835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_freq(cluster, rebuy_df, situation_df, people_df, spiritual_df):\n",
    "    add_info = []\n",
    "    for i in range(len(cluster)):\n",
    "        data = cluster['cluster_sentences'][i]\n",
    "        data = re.split(\",\", data)\n",
    "        freq_dic = {}                    # 所有关键词及其词频\n",
    "        feature_names = {}               # 前五关键词及其词频（倒序排列）\n",
    "        add_info.append({'psbe': \"\"})\n",
    "        \n",
    "        # 构造词频词典并排序输出前5个\n",
    "        for j in range(len(data)):\n",
    "            words=[]\n",
    "            words.extend([x for x in jieba.cut(data[j])])\n",
    "            for word in words:\n",
    "                if (word not in keys) | (word in ['小哥','一双','好多','速度']):\n",
    "                    continue\n",
    "                freq_dic[word]=freq_dic.get(word,0.0) + 1.0\n",
    "        for k,v in sorted(freq_dic.items(), key=lambda item: item[1], reverse=True)[:5]:\n",
    "            feature_names[k] = v / len(data)\n",
    "        add_info[-1]['freq_words'] = str(feature_names) # 给该段落打上前5关键词及其词频\n",
    "        # add_info[-1]['一级'] = keys_df.loc[keys.index(list(feature_names.keys())[0]),'一级'] # 给该段落打上第一关键词的一级\n",
    "        # add_info[-1]['二级'] = keys_df.loc[keys.index(list(feature_names.keys())[0]),'二级'] # 给该段落打上第一关键词的二级\n",
    "        \n",
    "        # 匹配复购、适用场景、人群、精神认同\n",
    "        cluster_word = list(feature_names.keys())\n",
    "        # 匹配复购\n",
    "        if len(set(cluster_word).intersection(set(rebuy_keyword))) > 0:\n",
    "            rebuy_df = pd.concat([rebuy_df, cluster[i:i+1]])\n",
    "            add_info[-1]['psbe'] = '复购'\n",
    "        # 匹配适用场景\n",
    "        if len(set(cluster_word).intersection(set(situation_keyword))) > 0:\n",
    "            situation_df = pd.concat([situation_df, cluster[i:i+1]])\n",
    "            add_info[-1]['psbe'] = '适用场景'\n",
    "        # 匹配人群\n",
    "        if len(set(cluster_word).intersection(set(people_keyword))) > 0:\n",
    "            people_df = pd.concat([people_df, cluster[i:i+1]])\n",
    "            add_info[-1]['psbe'] = '人群'\n",
    "        # 匹配精神认同\n",
    "        if len(set(cluster_word).intersection(set(spiritual_keyword))) > 0:\n",
    "            spiritual_df = pd.concat([spiritual_df, cluster[i:i+1]])\n",
    "            add_info[-1]['psbe'] = '精神认同'\n",
    "        \n",
    "        '''\n",
    "        # 打标签12345\n",
    "        # 1：1个tag_1,1个tag_2\n",
    "        # 2：1个tag_1,多个tag_2\n",
    "        # 3：多个tag_1\n",
    "        # 4：只去除or只保留特定词的句子（首个key的freq）\n",
    "        # 5：杂乱无章(首个key的freq）\n",
    "        '''\n",
    "        tag_result = []\n",
    "        for keyword, freq in feature_names.items():\n",
    "            if freq < 0.1:\n",
    "                break\n",
    "            if keyword in psbe_keyword:\n",
    "                continue\n",
    "            tag_result.append({'keyword': keyword,\n",
    "                               '一级': keys_df.loc[keys.index(keyword),'一级'],\n",
    "                               '二级': keys_df.loc[keys.index(keyword),'二级']})\n",
    "        tag_result = pd.DataFrame(tag_result)\n",
    "        if len(tag_result) == 0:\n",
    "            add_info[-1]['一级'] = '/'\n",
    "            add_info[-1]['二级'] = '/'\n",
    "            add_info[-1]['处理级别'] = 5\n",
    "            continue\n",
    "        add_info[-1]['一级'] = tag_result['一级'][0] # 给该段落打上第一关键词的一级\n",
    "        add_info[-1]['二级'] = tag_result['二级'][0] # 给该段落打上第一关键词的二级\n",
    "        # 首个关键词词频低于0.1则属于标签5\n",
    "        if feature_names[tag_result['keyword'][0]] < 0.2:\n",
    "            add_info[-1]['处理级别'] = 5\n",
    "            continue\n",
    "        # 判断是否只有一个tag_1，是则打标签 1 or 2\n",
    "        if len(tag_result['一级'].unique()) == 1: # 只有1个tag_1\n",
    "            if len(tag_result['二级'].unique()) == 1: # 只有一个tag_2\n",
    "                add_info[-1]['处理级别'] = 1 # 1个tag_1,1个tag_2则属于标签1\n",
    "                add_info[-1]['备注'] = ' '.join(tag_result['二级'].unique())\n",
    "            else: # 有多个tag_2\n",
    "                add_info[-1]['处理级别'] = 2 # 1个tag_1,多个tag_2则属于标签2\n",
    "                add_info[-1]['备注'] = ' '.join(tag_result['二级'].unique())\n",
    "        else: # 多个tag_1\n",
    "            if feature_names[tag_result['keyword'][0]] >= 0.8:    # 如果第一个词的词频很高，就判定为4\n",
    "                add_info[-1]['处理级别'] = 4\n",
    "                add_info[-1]['备注'] = ' '.join(tag_result['一级'].unique())\n",
    "            else:\n",
    "                add_info[-1]['处理级别'] = 3\n",
    "                add_info[-1]['备注'] = ' '.join(tag_result['一级'].unique())\n",
    "    \n",
    "    # 增加信息\n",
    "    add_info = pd.DataFrame(add_info).reset_index(drop=True)\n",
    "    cluster = cluster.join(add_info)\n",
    "        \n",
    "        \n",
    "    return cluster, rebuy_df, situation_df, people_df, spiritual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd847af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "词频小于0.1的关键词不要了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79929a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_df = pd.read_excel('C:/Users/DWu60/Desktop/sentiment/keywords_sentimental_words.xlsx', sheet_name='情感分析关键词')\n",
    "keys = list(keys_df['关键字'])\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e884fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuy_keyword_df = pd.read_excel('C:/Users/DWu60/Desktop/场景人群复购精神认同语料库.xlsx', sheet_name='rebuy')\n",
    "rebuy_keyword = list(rebuy_keyword_df['keyword'].unique())\n",
    "print(len(rebuy_keyword))\n",
    "rebuy_keyword[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c509089",
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_keyword_df = pd.read_excel('C:/Users/DWu60/Desktop/场景人群复购精神认同语料库.xlsx', sheet_name='situation')\n",
    "situation_keyword = list(situation_keyword_df['keyword'].unique())\n",
    "print(len(situation_keyword))\n",
    "situation_keyword[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_keyword_df = pd.read_excel('C:/Users/DWu60/Desktop/场景人群复购精神认同语料库.xlsx', sheet_name='people')\n",
    "people_keyword = list(people_keyword_df['keyword'].unique())\n",
    "print(len(people_keyword))\n",
    "people_keyword[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de64d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spiritual_keyword = ['代言', '王一博', '代言人', '龚俊', '徐坤', '李佳琦', '王一博而来', '佳琦', '张哲瀚', '宋亚轩']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de02e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "psbe_keyword = rebuy_keyword + situation_keyword + people_keyword + spiritual_keyword\n",
    "len(psbe_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuy_df = pd.DataFrame()\n",
    "situation_df = pd.DataFrame()\n",
    "people_df = pd.DataFrame()\n",
    "spiritual_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba4c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_positive = pd.read_excel('C:/Users/DWu60/Desktop/Case1_NLP/result_2.0/cluster/classify_sample_1_positive_cos_kmeans_ascending_275.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, rebuy_df, situation_df, people_df, spiritual_df = cal_freq(cluster_positive, rebuy_df, situation_df, people_df, spiritual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51ee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
