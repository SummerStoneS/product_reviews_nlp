{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fb60ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\rshe11\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.205 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from snownlp import sentiment, SnowNLP\n",
    "import re\n",
    "import nlpir\n",
    "from langconv import Converter\n",
    "import glob\n",
    "from utils import get_source_comments_data, read_list, sepSentences, GetWordDict, filter_cutting_sentence\n",
    "from my_sentiment import cal_score, keyword_tag\n",
    "from my_sentence_cut import cut_sentences\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90025989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snownlp给每条评论计算情感分数\n",
    "def classify_sentiment(unique_paragraphs):\n",
    "    positive_paragraphs = []\n",
    "    negative_paragraphs = []\n",
    "    neutral_paragraphs = []\n",
    "    paragraph_sentiment_list = []\n",
    "    for par in tqdm(unique_paragraphs):\n",
    "        s_par = SnowNLP(par)\n",
    "        if s_par.sentiments > 0.6:\n",
    "            positive_paragraphs.append(par)\n",
    "            s = 'positive'\n",
    "        elif s_par.sentiments < 0.3:\n",
    "            negative_paragraphs.append(par)\n",
    "            s = 'negative'\n",
    "        else:\n",
    "            neutral_paragraphs.append(par)\n",
    "            s = 'neutral'\n",
    "        paragraph_sentiment_list.append((par, s_par.sentiments, s))\n",
    "    return positive_paragraphs, negative_paragraphs, neutral_paragraphs, paragraph_sentiment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70db3dc",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf467b80",
   "metadata": {},
   "source": [
    "#### 读取原始评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f568a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始评价数： 911098\n",
      "原始不为空评论数: 911057\n"
     ]
    }
   ],
   "source": [
    "dataset = get_source_comments_data(is_first_time=False)\n",
    "print(\"原始评价数：\", len(dataset))\n",
    "comments_list = dataset[\"评论内容\"].dropna().tolist()\n",
    "print(\"原始不为空评论数:\", len(comments_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cafd9",
   "metadata": {},
   "source": [
    "#### Step1 给每条评论断句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bef1917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "断句后的段落数: 3179427\n",
      "断句后的非重复段落数： 1192574\n"
     ]
    }
   ],
   "source": [
    "# 简单断句--标点符号转折语断句\n",
    "sentences = sepSentences(comments_list)\n",
    "print(\"断句后的段落数:\", len(sentences))\n",
    "unique_sentences = pd.Series(sentences).drop_duplicates().tolist()\n",
    "print(\"断句后的非重复段落数：\", len(unique_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ea5445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\AppData\\Local\\Temp/ipykernel_59308/2713843231.py:17: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  keyword_dict = keyword.T.to_dict('dict')\n"
     ]
    }
   ],
   "source": [
    "# 用断句模型断句\n",
    "# 1.挑出来一部分符合条件的断句\n",
    "sentence_to_cut, remain = filter_cutting_sentence(unique_sentences)\n",
    "pd.Series(remain).to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='remain')\n",
    "pd.Series(sentence_to_cut).to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='sentence_to_cut')\n",
    "# 2.调用断句模型断句\n",
    "model_path = \"断句/nike_comment-master/checkpoint/wwm_conv1d-all-000_modeling_comments.pkl\"\n",
    "bert_name = 'hfl/chinese-bert-wwm-ext'\n",
    "cut_result = cut_sentences(sentence_to_cut, model_path, bert_name)\n",
    "cut_result.to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='cut_result')\n",
    "sentence_to_cut = itertools.chain(*cut_result['cut_result'].tolist())\n",
    "sentence_to_cut = [sentence for sentence in sentence_to_cut if (re.search('[\\u4e00-\\u9fa5]', sentence) != None) and (len(sentence) > 1)]\n",
    "remain.extend(sentence_to_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83619f5d",
   "metadata": {},
   "source": [
    "#### Step2 snownlp情感判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37db554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首次运行并保存结果 时间很长\n",
    "positive, negative, neutral, paragraph_sentiment_list = classify_sentiment(remain)\n",
    "pd.DataFrame(paragraph_sentiment_list, columns=[\"text\",'senti_score','sentiment']).to_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab40792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取结果\n",
    "paragraph_sentiment_list = pd.read_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\") # snownlp的情感分析结果\n",
    "positive = paragraph_sentiment_list.query(\"sentiment=='positive'\")\n",
    "negative = paragraph_sentiment_list.query(\"sentiment=='negative'\")\n",
    "neutral = paragraph_sentiment_list.query(\"sentiment=='neutral'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df05347",
   "metadata": {},
   "source": [
    "#### Step3 修正情感判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4967e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中性 201725\n",
      "正面 472914\n",
      "负面 254406\n"
     ]
    }
   ],
   "source": [
    "# 自己设计的情感分析算法给语句分正负面中性\n",
    "neg_result, neg_nokeywords = cal_score(negative[\"text\"].to_list(), orisen_tag=\"负面\")\n",
    "print(\"中性\", len(neg_result))\n",
    "pos_result, pos_nokeywords = cal_score(positive[\"text\"].to_list(), orisen_tag=\"正面\")\n",
    "print('正面', len(pos_result))\n",
    "neu_result, neu_nokeywords = cal_score(neutral[\"text\"].to_list(), orisen_tag=\"中性\")\n",
    "print('负面', len(neu_result))\n",
    "result = pd.concat([neg_result, neu_result, pos_result, neg_nokeywords, pos_nokeywords, pos_nokeywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0138d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把复购，人群，场景，精神认同的先筛选出来不进入到聚类了\n",
    "psbe = result[result['tag_1'].isin(['复购', '人群', '适用场景', '精神认同'])]\n",
    "psbe.to_excel(r\"model_result/sentiment/rebuy_people_situation_endorsement.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b78e52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[~result['tag_1'].isin(['复购', '人群', '适用场景', '精神认同'])]\n",
    "my_sentiment_result = result[result[\"keyword\"].notnull()]  \n",
    "nokeyword_result = result[result[\"keyword\"].isnull()]      # 没有keyword的句子，包括只有comment的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "080d998f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd17831c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>orisen_tag</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keyword</th>\n",
       "      <th>tag_1</th>\n",
       "      <th>tag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484507</th>\n",
       "      <td>这个包装让我感觉这双鞋不像是正品</td>\n",
       "      <td>包装 感觉 这双鞋 不 像是 正品</td>\n",
       "      <td>正面</td>\n",
       "      <td>0.5</td>\n",
       "      <td>中性</td>\n",
       "      <td>包装</td>\n",
       "      <td>服务</td>\n",
       "      <td>包装</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484508</th>\n",
       "      <td>这个包装让我感觉这双鞋不像是正品</td>\n",
       "      <td>包装 感觉 这双鞋 不 像是 正品</td>\n",
       "      <td>正面</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>负面</td>\n",
       "      <td>正品</td>\n",
       "      <td>做工</td>\n",
       "      <td>正品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484510</th>\n",
       "      <td>价格款式还比较满意</td>\n",
       "      <td>价格 款式 还 比较满意</td>\n",
       "      <td>正面</td>\n",
       "      <td>1.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>价格</td>\n",
       "      <td>价格</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484511</th>\n",
       "      <td>价格款式还比较满意</td>\n",
       "      <td>价格 款式 还 比较满意</td>\n",
       "      <td>正面</td>\n",
       "      <td>1.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>款式</td>\n",
       "      <td>好看</td>\n",
       "      <td>款式</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484512</th>\n",
       "      <td>比去年贵了好多啊</td>\n",
       "      <td>比 去年 贵 好多</td>\n",
       "      <td>正面</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>负面</td>\n",
       "      <td>贵</td>\n",
       "      <td>价格</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sentence       segmentation orisen_tag  score sentiment  \\\n",
       "484507  这个包装让我感觉这双鞋不像是正品  包装 感觉 这双鞋 不 像是 正品         正面    0.5        中性   \n",
       "484508  这个包装让我感觉这双鞋不像是正品  包装 感觉 这双鞋 不 像是 正品         正面   -2.0        负面   \n",
       "484510         价格款式还比较满意       价格 款式 还 比较满意         正面    1.0        正面   \n",
       "484511         价格款式还比较满意       价格 款式 还 比较满意         正面    1.0        正面   \n",
       "484512          比去年贵了好多啊          比 去年 贵 好多         正面   -1.0        负面   \n",
       "\n",
       "       keyword tag_1 tag_2  \n",
       "484507      包装    服务    包装  \n",
       "484508      正品    做工    正品  \n",
       "484510      价格    价格     /  \n",
       "484511      款式    好看    款式  \n",
       "484512       贵    价格     /  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentiment_result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb2f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\AppData\\Local\\Temp\\1/ipykernel_34824/307503491.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nokeyword_result['sentiment'] = np.where(nokeyword_result['sentiment'] == '正面', nokeyword_result['orisen_tag'], nokeyword_result['sentiment'])\n"
     ]
    }
   ],
   "source": [
    "# # 检查自主开发的情感分析和snownlp结果不一样的\n",
    "# mismatch=nokeyword_result[(nokeyword_result['sentiment'] != nokeyword_result['orisen_tag'])&nokeyword_result['sentiment'].notnull()]\n",
    "# mismatch.to_excel(\"check/mismatch_sentiment.xlsx\", index=False)\n",
    "# 只有comment的句子，my sentiment是中性和负面的还用自己的，正面的用snownlp的\n",
    "nokeyword_result['sentiment'] = np.where(nokeyword_result['sentiment'] == '正面', nokeyword_result['orisen_tag'], nokeyword_result['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b203a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2703: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['sentence', 'segmentation', 'sentiment', 'orisen_tag', 'keyword',\n",
      "       'tag_1', 'tag_2'],\n",
      "      dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "# 保存结果\n",
    "my_sentiment_result.to_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"keyword\")  # 有分数的为有keyword或者no_keyword里有情感词的即可以计算分数的\n",
    "my_sentiment_result.to_csv(r\"model_result/sentiment/comments_sentiment_keywords.csv\", index=False, encoding='utf-8')\n",
    "nokeyword_result.to_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"no_keyword\")\n",
    "nokeyword_result.to_csv(r\"model_result/sentiment/comments_sentiment_no_keywords.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd0a46",
   "metadata": {},
   "source": [
    "#### Step4 每类情感生成一个文件，为后续聚类做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "146a13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_useless_neutral_sentences(my_sentiment_result):\n",
    "    \"\"\"\n",
    "        如果一句话已经有正面或者负面，那就没必要再在中性里出现了\n",
    "    \"\"\"\n",
    "    sentence_sentiment_num = my_sentiment_result.groupby(['sentence','sentiment'])['keyword'].count()\n",
    "    sentence_sentiment_num = sentence_sentiment_num.unstack()\n",
    "    return set(sentence_sentiment_num[((sentence_sentiment_num['正面'] > 0)|(sentence_sentiment_num['负面'] > 0)) & (sentence_sentiment_num['中性'] > 0)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7723f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面的句子数量：275892\n",
      "负面的句子数量：117384\n",
      "中性的句子数量：203001\n",
      "no keyword no score\n",
      "正面 84300\n",
      "负面 137085\n",
      "中性 57219\n"
     ]
    }
   ],
   "source": [
    "my_sentiment_result = pd.read_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"keyword\")\n",
    "sentiment_en = {'正面': 'positive', '负面': 'negative', '中性':'neutral'}\n",
    "for sentiment_type in ['正面', '负面', '中性']: \n",
    "    sentiments_data = my_sentiment_result[my_sentiment_result[\"sentiment\"] == sentiment_type]\n",
    "    sentiments_data = sentiments_data[\"sentence\"].unique().tolist()\n",
    "    print(f\"{sentiment_type}的句子数量：{len(sentiments_data)}\")\n",
    "    if sentiment_type == '中性':\n",
    "        remove_sentences = filter_useless_neutral_sentences(my_sentiment_result)\n",
    "        sentiments_data = [sentence for sentence in set(sentiments_data) if sentence not in remove_sentences]\n",
    "    simplified_sentences = [sentence for sentence in sentiments_data if ((len(sentence) <= 50) and (len(sentence) >= 2))]\n",
    "    with open(f\"model_result/sentiment/{sentiment_en[sentiment_type]}_sentence_list.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(simplified_sentences))\n",
    "        \n",
    "nokeyword_result = pd.read_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"no_keyword\")\n",
    "print(\"no keyword\")\n",
    "for sentiment_type in ['正面', '负面', '中性']:\n",
    "    no_keyword_data = nokeyword_result.query(\"sentiment == @sentiment_type\")\n",
    "    others = no_keyword_data[\"sentence\"].unique().tolist()\n",
    "    print(sentiment_type, len(others))\n",
    "    simplified_sentences = [sentence for sentence in others if ((len(sentence) <= 50) and (len(sentence) >= 2))]\n",
    "    with open(f\"model_result/sentiment/no_keyword_{sentiment_en[sentiment_type]}_sentence_list.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(simplified_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852901d",
   "metadata": {},
   "source": [
    "#####################         end         ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec1c69ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(681968, 8)\n",
      "(23592, 8)\n"
     ]
    }
   ],
   "source": [
    "print(my_sentiment_result[my_sentiment_result['sentence'].str.len() <= 20].shape)\n",
    "print(my_sentiment_result[my_sentiment_result['sentence'].str.len() > 20].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1e7b0",
   "metadata": {},
   "source": [
    "my_sentiment_result[my_sentiment_result['sentence'].str.len() > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21345e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in long_sentence.index.to_list():\n",
    "        f.write(str(sentence))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_sentiment_num2[(sentence_sentiment_num2['正面'] > 0) & (sentence_sentiment_num2['中性'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c760bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"from_Doris/comments_sentiment_result_with_tag_ABCDEF.hdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
