{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fb60ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24248\\3513441817.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msnownlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSnowNLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from snownlp import sentiment, SnowNLP\n",
    "import re\n",
    "import nlpir\n",
    "from langconv import Converter\n",
    "import glob\n",
    "from utils import get_source_comments_data, read_list, sepSentences, GetWordDict, filter_cutting_sentence\n",
    "from my_sentiment import cal_score, keyword_tag\n",
    "from my_sentence_cut import cut_sentences\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90025989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snownlp给每条评论计算情感分数\n",
    "def classify_sentiment(unique_paragraphs):\n",
    "    positive_paragraphs = []\n",
    "    negative_paragraphs = []\n",
    "    neutral_paragraphs = []\n",
    "    paragraph_sentiment_list = []\n",
    "    for par in tqdm(unique_paragraphs):\n",
    "        s_par = SnowNLP(par)\n",
    "        if s_par.sentiments > 0.6:\n",
    "            positive_paragraphs.append(par)\n",
    "            s = 'positive'\n",
    "        elif s_par.sentiments < 0.3:\n",
    "            negative_paragraphs.append(par)\n",
    "            s = 'negative'\n",
    "        else:\n",
    "            neutral_paragraphs.append(par)\n",
    "            s = 'neutral'\n",
    "        paragraph_sentiment_list.append((par, s_par.sentiments, s))\n",
    "    return positive_paragraphs, negative_paragraphs, neutral_paragraphs, paragraph_sentiment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70db3dc",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf467b80",
   "metadata": {},
   "source": [
    "#### 读取原始评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f568a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始评价数： 911098\n",
      "原始不为空评论数: 911057\n"
     ]
    }
   ],
   "source": [
    "dataset = get_source_comments_data(is_first_time=False)\n",
    "print(\"原始评价数：\", len(dataset))\n",
    "comments_list = dataset[\"评论内容\"].dropna().tolist()\n",
    "print(\"原始不为空评论数:\", len(comments_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cafd9",
   "metadata": {},
   "source": [
    "#### Step1 给每条评论断句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bef1917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "断句后的段落数: 3179427\n",
      "断句后的非重复段落数： 1192574\n"
     ]
    }
   ],
   "source": [
    "# 简单断句--标点符号转折语断句\n",
    "sentences = sepSentences(comments_list)\n",
    "print(\"断句后的段落数:\", len(sentences))\n",
    "unique_sentences = pd.Series(sentences).drop_duplicates().tolist()\n",
    "print(\"断句后的非重复段落数：\", len(unique_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3813d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\AppData\\Local\\Temp/ipykernel_59308/2713843231.py:17: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  keyword_dict = keyword.T.to_dict('dict')\n"
     ]
    }
   ],
   "source": [
    "# 用断句模型断句\n",
    "# 1.挑出来一部分符合条件的断句\n",
    "sentence_to_cut, remain = filter_cutting_sentence(unique_sentences)\n",
    "pd.Series(remain).to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='remain')\n",
    "pd.Series(sentence_to_cut).to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='sentence_to_cut')\n",
    "# 2.调用断句模型断句\n",
    "model_path = \"断句/nike_comment-master/checkpoint/wwm_conv1d-all-000_modeling_comments.pkl\"\n",
    "bert_name = 'hfl/chinese-bert-wwm-ext'\n",
    "cut_result = cut_sentences(sentence_to_cut, model_path, bert_name)\n",
    "cut_result.to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='cut_result')\n",
    "sentence_to_cut = itertools.chain(*cut_result['cut_result'].tolist())\n",
    "sentence_to_cut = [sentence for sentence in sentence_to_cut if (re.search('[\\u4e00-\\u9fa5]', sentence) != None) and (len(sentence) > 1)]\n",
    "remain.extend(sentence_to_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83619f5d",
   "metadata": {},
   "source": [
    "#### Step2 snownlp情感判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37db554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首次运行并保存结果 时间很长\n",
    "positive, negative, neutral, paragraph_sentiment_list = classify_sentiment(remain)\n",
    "pd.DataFrame(paragraph_sentiment_list, columns=[\"text\",'senti_score','sentiment']).to_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ab40792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取结果\n",
    "paragraph_sentiment_list = pd.read_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\") # snownlp的情感分析结果\n",
    "positive = paragraph_sentiment_list.query(\"sentiment=='positive'\")\n",
    "negative = paragraph_sentiment_list.query(\"sentiment=='negative'\")\n",
    "neutral = paragraph_sentiment_list.query(\"sentiment=='neutral'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df05347",
   "metadata": {},
   "source": [
    "#### Step3 修正情感判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4967e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己设计的情感分析算法给语句分正负面中性\n",
    "neg_result, neg_nokeywords = cal_score(negative[\"text\"].to_list(), orisen_tag=\"负面\")\n",
    "print(\"中性\", len(neg_result))\n",
    "pos_result, pos_nokeywords = cal_score(positive[\"text\"].to_list(), orisen_tag=\"正面\")\n",
    "print('正面', len(pos_result))\n",
    "neu_result, neu_nokeywords = cal_score(neutral[\"text\"].to_list(), orisen_tag=\"中性\")\n",
    "print('负面', len(neu_result))\n",
    "result = pd.concat([neg_result, neu_result, pos_result, neg_nokeywords, pos_nokeywords, pos_nokeywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26970e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把复购，人群，场景，精神认同的先筛选出来不进入到聚类了\n",
    "psbe = result[result['tag_1'].isin(['复购', '人群', '适用场景', '精神认同'])]\n",
    "psbe.to_excel(r\"model_result/sentiment/rebuy_people_situation_endorsement.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1524637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_result = result[~result['tag_1'].isin(['复购', '人群', '适用场景', '精神认同'])]\n",
    "my_sentiment_result = normal_result[normal_result[\"keyword\"].notnull()]  \n",
    "nokeyword_result = normal_result[normal_result[\"keyword\"].isnull()]      # 没有keyword的句子，包括只有comment的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d56efeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>orisen_tag</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keyword</th>\n",
       "      <th>tag_1</th>\n",
       "      <th>tag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>透气性稍差</td>\n",
       "      <td>透气性 稍差</td>\n",
       "      <td>负面</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>负面</td>\n",
       "      <td>透气性</td>\n",
       "      <td>舒适</td>\n",
       "      <td>透气</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>有些人说硬</td>\n",
       "      <td>说 硬</td>\n",
       "      <td>负面</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>负面</td>\n",
       "      <td>硬</td>\n",
       "      <td>舒适</td>\n",
       "      <td>脚感软</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>鞋子穿上不磨脚</td>\n",
       "      <td>鞋子 穿 不磨脚</td>\n",
       "      <td>负面</td>\n",
       "      <td>2.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>不磨脚</td>\n",
       "      <td>舒适</td>\n",
       "      <td>磨脚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>不臭脚</td>\n",
       "      <td>不 臭脚</td>\n",
       "      <td>负面</td>\n",
       "      <td>1.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>臭脚</td>\n",
       "      <td>舒适</td>\n",
       "      <td>透气</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>并不是特别软</td>\n",
       "      <td>不是 特别 软</td>\n",
       "      <td>负面</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>负面</td>\n",
       "      <td>软</td>\n",
       "      <td>舒适</td>\n",
       "      <td>脚感软</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472873</th>\n",
       "      <td>挺轻</td>\n",
       "      <td>挺轻</td>\n",
       "      <td>正面</td>\n",
       "      <td>2.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>挺轻</td>\n",
       "      <td>舒适</td>\n",
       "      <td>轻巧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472874</th>\n",
       "      <td>舒服</td>\n",
       "      <td>舒服</td>\n",
       "      <td>正面</td>\n",
       "      <td>2.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>舒服</td>\n",
       "      <td>舒适</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472876</th>\n",
       "      <td>连买了四双鞋子铁轻巧</td>\n",
       "      <td>连买 四双 鞋子 铁 轻巧</td>\n",
       "      <td>正面</td>\n",
       "      <td>2.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>轻巧</td>\n",
       "      <td>舒适</td>\n",
       "      <td>轻巧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472879</th>\n",
       "      <td>穿着有舒服</td>\n",
       "      <td>穿着 有 舒服</td>\n",
       "      <td>正面</td>\n",
       "      <td>4.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>舒服</td>\n",
       "      <td>舒适</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472886</th>\n",
       "      <td>舒适合脚</td>\n",
       "      <td>舒适 合脚</td>\n",
       "      <td>正面</td>\n",
       "      <td>2.0</td>\n",
       "      <td>正面</td>\n",
       "      <td>舒适</td>\n",
       "      <td>舒适</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173088 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentence   segmentation orisen_tag  score sentiment keyword tag_1  \\\n",
       "1            透气性稍差         透气性 稍差         负面   -0.5        负面     透气性    舒适   \n",
       "4            有些人说硬            说 硬         负面   -1.0        负面       硬    舒适   \n",
       "8          鞋子穿上不磨脚       鞋子 穿 不磨脚         负面    2.0        正面     不磨脚    舒适   \n",
       "9              不臭脚           不 臭脚         负面    1.0        正面      臭脚    舒适   \n",
       "10          并不是特别软        不是 特别 软         负面   -2.0        负面       软    舒适   \n",
       "...            ...            ...        ...    ...       ...     ...   ...   \n",
       "472873          挺轻             挺轻         正面    2.0        正面      挺轻    舒适   \n",
       "472874          舒服             舒服         正面    2.0        正面      舒服    舒适   \n",
       "472876  连买了四双鞋子铁轻巧  连买 四双 鞋子 铁 轻巧         正面    2.0        正面      轻巧    舒适   \n",
       "472879       穿着有舒服        穿着 有 舒服         正面    4.0        正面      舒服    舒适   \n",
       "472886        舒适合脚          舒适 合脚         正面    2.0        正面      舒适    舒适   \n",
       "\n",
       "       tag_2  \n",
       "1         透气  \n",
       "4        脚感软  \n",
       "8         磨脚  \n",
       "9         透气  \n",
       "10       脚感软  \n",
       "...      ...  \n",
       "472873    轻巧  \n",
       "472874     /  \n",
       "472876    轻巧  \n",
       "472879     /  \n",
       "472886     /  \n",
       "\n",
       "[173088 rows x 8 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentiment_result.query(\"tag_1 == '舒适'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bb2f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\AppData\\Local\\Temp/ipykernel_11056/4216519342.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nokeyword_result['sentiment'] = np.where((nokeyword_result['sentiment'] == '正面') & (nokeyword_result['orisen_tag'].isin(['负面','中性'])), '中性', nokeyword_result['sentiment'])\n"
     ]
    }
   ],
   "source": [
    "# # 检查自主开发的情感分析和snownlp结果不一样的\n",
    "# mismatch=nokeyword_result[(nokeyword_result['sentiment'] != nokeyword_result['orisen_tag']) & nokeyword_result['sentiment'].notnull()]\n",
    "# mismatch.to_excel(\"check/mismatch_sentiment_v3.xlsx\", index=False)\n",
    "\n",
    "# 只有comment的句子，my sentiment是中性和负面的还用自己的，正面的用snownlp的,但是\n",
    "nokeyword_result['sentiment'] = np.where((nokeyword_result['sentiment'] == '正面') & (nokeyword_result['orisen_tag'].isin(['负面','中性'])), '中性', nokeyword_result['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b203a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2703: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['sentence', 'segmentation', 'orisen_tag', 'sentiment', 'keyword',\n",
      "       'tag_1', 'tag_2'],\n",
      "      dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "# 保存结果\n",
    "my_sentiment_result.to_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"keyword\")  # 有分数的为有keyword或者no_keyword里有情感词的即可以计算分数的\n",
    "my_sentiment_result.to_csv(r\"model_result/sentiment/comments_sentiment_keywords.csv\", index=False, encoding='utf-8')\n",
    "nokeyword_result.to_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"no_keyword\")\n",
    "nokeyword_result.to_csv(r\"model_result/sentiment/comments_sentiment_no_keywords.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd0a46",
   "metadata": {},
   "source": [
    "#### Step4 每类情感生成一个文件，为后续聚类做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "146a13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_useless_neutral_sentences(my_sentiment_result):\n",
    "    \"\"\"\n",
    "        如果一句话已经有正面或者负面，那就没必要再在中性里出现了\n",
    "    \"\"\"\n",
    "    sentence_sentiment_num = my_sentiment_result.groupby(['sentence','sentiment'])['keyword'].count()\n",
    "    sentence_sentiment_num = sentence_sentiment_num.unstack()\n",
    "    return set(sentence_sentiment_num[((sentence_sentiment_num['正面'] > 0)|(sentence_sentiment_num['负面'] > 0)) & (sentence_sentiment_num['中性'] > 0)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7723f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面的句子数量：212047\n",
      "负面的句子数量：104587\n",
      "中性的句子数量：80929\n",
      "no keyword\n",
      "正面 86210\n",
      "负面 111211\n",
      "中性 86566\n"
     ]
    }
   ],
   "source": [
    "my_sentiment_result = pd.read_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"keyword\")\n",
    "sentiment_en = {'正面': 'positive', '负面': 'negative', '中性':'neutral'}\n",
    "for sentiment_type in ['正面', '负面', '中性']: \n",
    "    sentiments_data = my_sentiment_result[my_sentiment_result[\"sentiment\"] == sentiment_type]\n",
    "    sentiments_data = sentiments_data[\"sentence\"].unique().tolist()\n",
    "    print(f\"{sentiment_type}的句子数量：{len(sentiments_data)}\")\n",
    "    if sentiment_type == '中性':\n",
    "        remove_sentences = filter_useless_neutral_sentences(my_sentiment_result)\n",
    "        sentiments_data = [sentence for sentence in set(sentiments_data) if sentence not in remove_sentences]\n",
    "    simplified_sentences = [sentence for sentence in sentiments_data if ((len(sentence) <= 50) and (len(sentence) >= 2))]\n",
    "    with open(f\"model_result/sentiment/{sentiment_en[sentiment_type]}_sentence_list.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(simplified_sentences))\n",
    "        \n",
    "nokeyword_result = pd.read_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"no_keyword\")\n",
    "print(\"no keyword\")\n",
    "for sentiment_type in ['正面', '负面', '中性']:\n",
    "    no_keyword_data = nokeyword_result.query(\"sentiment == @sentiment_type\")\n",
    "    others = no_keyword_data[\"sentence\"].unique().tolist()\n",
    "    print(sentiment_type, len(others))\n",
    "    simplified_sentences = [sentence for sentence in others if ((len(sentence) <= 50) and (len(sentence) >= 2))]\n",
    "    with open(f\"model_result/sentiment/no_keyword_{sentiment_en[sentiment_type]}_sentence_list.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(simplified_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852901d",
   "metadata": {},
   "source": [
    "#####################         end         ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec1c69ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(681968, 8)\n",
      "(23592, 8)\n"
     ]
    }
   ],
   "source": [
    "print(my_sentiment_result[my_sentiment_result['sentence'].str.len() <= 20].shape)\n",
    "print(my_sentiment_result[my_sentiment_result['sentence'].str.len() > 20].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1e7b0",
   "metadata": {},
   "source": [
    "my_sentiment_result[my_sentiment_result['sentence'].str.len() > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21345e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in long_sentence.index.to_list():\n",
    "        f.write(str(sentence))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_sentiment_num2[(sentence_sentiment_num2['正面'] > 0) & (sentence_sentiment_num2['中性'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c760bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"from_Doris/comments_sentiment_result_with_tag_ABCDEF.hdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
