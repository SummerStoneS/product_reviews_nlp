{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec096f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f77e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list(path):\n",
    "    words = [line.strip() for line in open(path, encoding='UTF-8').readlines()]\n",
    "    words = np.unique(words, axis=0) # 删除重复数据\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae443d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = r'model_input\\keywords\\cn_stopwords.txt'\n",
    "stopwords = read_list(stopwords_path)\n",
    "remove = np.array(['好','不','不是','可','如果','不知','有','不如','一般','再','可以','还要','要','便于','也','又','还','比','和'])\n",
    "stopwords = np.setdiff1d(stopwords,remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20d81d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_input_file = r'model_input\\keywords\\keywords_sentimental_words.xlsx'\n",
    "cluster_input_file = r'场景人群代言复购语料库/场景人群复购精神认同语料库4.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbbff0",
   "metadata": {},
   "source": [
    "# 一、精神认同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3329f36",
   "metadata": {},
   "source": [
    "### 1.1 明星代言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "133cb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_endorsement(df, column_name='段落'):\n",
    "    \n",
    "    # 自定义明星代言去除词汇\n",
    "    dropwords = ['双十一','视频','二代','硝化细菌','正品店铺','态度好','广场舞','跑步','耐心',\n",
    "       '发微信','独具慧眼','笔名病魔改名','良心商家','值得回购','淘宝首页','中底科技革命','纳爱斯集团']\n",
    "    defined_words = ['直播间','佳琦']                 # 李佳琦直播的特殊处理\n",
    "    special_words = dropwords + defined_words        # 所有特殊词汇\n",
    "    useless = []                                     # 其他\n",
    "    lijiaqi = []                                     # 李佳琦直播间相关语句   \n",
    "    endorsement_result = []                          # 明星代言清理结果\n",
    "    \n",
    "    \n",
    "    def endorsement_parser(row):\n",
    "        \"\"\"\n",
    "            row:一个类里所有语句\n",
    "            return:符合需求的一个明星代言类\n",
    "        \"\"\"\n",
    "        endorsements = []                            # 常规明星代言相关语句\n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            if all(sentence.find(word)== -1 for word in special_words):\n",
    "                endorsements.append(sentence)\n",
    "            elif (sentence.find('代言') >= 0) | (sentence.find('粉丝') >= 0):\n",
    "                endorsements.append(sentence)\n",
    "            elif all(sentence.find(word) == -1 for word in defined_words):\n",
    "                useless.append(sentence)                                  \n",
    "            else:\n",
    "                lijiaqi.append(sentence)\n",
    "\n",
    "        return ['精神认同', '明星代言', len(endorsements), \",\".join(endorsements)]\n",
    "    \n",
    "\n",
    "    for row in df[column_name]:\n",
    "        cleaned_rows = endorsement_parser(row)\n",
    "        endorsement_result.append(cleaned_rows)\n",
    "    endorsement_result.append(['精神认同', '李佳琦', len(lijiaqi), \",\".join(lijiaqi)])\n",
    "    endorsement_result.append(['精神认同', 'useless', len(useless), \",\".join(useless)])    \n",
    "    return pd.DataFrame(endorsement_result, columns=[\"tag1\", 'tag2', '评论数', '段落'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de4e32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 明星代言处理\n",
    "data = pd.read_excel(cluster_input_file, sheet_name='精神认同')\n",
    "endorsement = data[data['tag_2'] == '明星代言']\n",
    "endorsement_result = clean_endorsement(endorsement, column_name='段落')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ec35bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>465</td>\n",
       "      <td>因为黄景瑜代言fila果断下单, 耐克宣称王一博为官方合作伙伴, 跟宋亚轩穿同款好开心, 小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>604</td>\n",
       "      <td>代言, 时代少年团代言了, zici时代少年团的代言, 自从俊代言以后, 这次宣了俊代言, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>549</td>\n",
       "      <td>主持龚俊, 时代少年团宋亚轩, 永远期待宋亚轩, 永远支持宋亚轩, 也会一直支持龚俊, 本来...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>611</td>\n",
       "      <td>听到消息代言真的是又激动又开心, 和代言人一起加油哦, 顺便说一下代言人真的很帅哈, 和代言...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>583</td>\n",
       "      <td>博君一肖百香果为王一博, 龚俊, 博君一肖为王一博而来, 顺便说一句我爱龚俊, 博君一肖百香...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>569</td>\n",
       "      <td>支持王一博代言颜色真滴好看, 小飞侠支持肖战代言鞋子超好看, 我家哥很喜欢百香果支持新代言人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>明星代言</td>\n",
       "      <td>519</td>\n",
       "      <td>肖战代言以后买了几件李宁产品,王一博代言安踏后线上线下买了不少安踏的产品,因为肖战代言开始入...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>李佳琦</td>\n",
       "      <td>138</td>\n",
       "      <td>佳琦直播间优惠力度太大了, 之前李佳琦直播间609入了一双小码数的, 佳琦直播间下单, 佳琦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>useless</td>\n",
       "      <td>4</td>\n",
       "      <td>因为态极科技的成功引发了国内运动品牌全行业的中底科技革命,因为态极科技的成功引发了国内运动...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tag1     tag2  评论数                                                 段落\n",
       "0  精神认同     明星代言  465  因为黄景瑜代言fila果断下单, 耐克宣称王一博为官方合作伙伴, 跟宋亚轩穿同款好开心, 小...\n",
       "1  精神认同     明星代言  604  代言, 时代少年团代言了, zici时代少年团的代言, 自从俊代言以后, 这次宣了俊代言, ...\n",
       "2  精神认同     明星代言  549  主持龚俊, 时代少年团宋亚轩, 永远期待宋亚轩, 永远支持宋亚轩, 也会一直支持龚俊, 本来...\n",
       "3  精神认同     明星代言  611  听到消息代言真的是又激动又开心, 和代言人一起加油哦, 顺便说一下代言人真的很帅哈, 和代言...\n",
       "4  精神认同     明星代言  583  博君一肖百香果为王一博, 龚俊, 博君一肖为王一博而来, 顺便说一句我爱龚俊, 博君一肖百香...\n",
       "5  精神认同     明星代言  569  支持王一博代言颜色真滴好看, 小飞侠支持肖战代言鞋子超好看, 我家哥很喜欢百香果支持新代言人...\n",
       "6  精神认同     明星代言    0                                                   \n",
       "7  精神认同     明星代言  519  肖战代言以后买了几件李宁产品,王一博代言安踏后线上线下买了不少安踏的产品,因为肖战代言开始入...\n",
       "8  精神认同      李佳琦  138  佳琦直播间优惠力度太大了, 之前李佳琦直播间609入了一双小码数的, 佳琦直播间下单, 佳琦...\n",
       "9  精神认同  useless    4   因为态极科技的成功引发了国内运动品牌全行业的中底科技革命,因为态极科技的成功引发了国内运动..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endorsement_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f7c7c",
   "metadata": {},
   "source": [
    "### 1.2 国货"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc68ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domestic(df, column_name='段落'):\n",
    "    \n",
    "    # 自定义国货搜索词\n",
    "    brand = ['anta','do-win','erke','lining','peak','xtep','安踏','匹克','鸿星尔克','李宁','多威','特步','361','乔丹','dowin',\n",
    "       '国货','国牌','国产','国内品牌','国民品牌','国有品牌','我国的品牌','国潮','国内的','中国','我们国家','自己国家',\n",
    "       '自主品牌','民族品牌','国鞋','本土品牌','自己的品牌','国内制造','国家的品牌','国家产品','尔克','国乔','中乔','奇弹','太极','态极']\n",
    "    no_words = ['差','不敢恭维','不是','武汉加油']  # 否定词语\n",
    "    domestic_result = []                          # 国货清理结果\n",
    "    useless = []                                  # 其他\n",
    "    \n",
    "    \n",
    "    def domestic_parser(row):\n",
    "        \"\"\"\n",
    "            row:一个类里所有语句\n",
    "            return:符合需求的一个国货类\n",
    "        \"\"\"\n",
    "        domestics = []                            # 常规国货相关语句\n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            if any(sentence.lower().find(word) >= 0 for word in brand):\n",
    "                if any(sentence.lower().find(word) >= 0 for word in no_words):\n",
    "                    useless.append(sentence)\n",
    "                else:\n",
    "                    domestics.append(sentence)\n",
    "            else:\n",
    "                useless.append(sentence)\n",
    "        \n",
    "        return ['精神认同', '国货', len(domestics), \",\".join(domestics)]\n",
    "    \n",
    "    \n",
    "    for row in df[column_name]:\n",
    "        cleaned_rows = domestic_parser(row)\n",
    "        domestic_result.append(cleaned_rows)\n",
    "    domestic_result.append(['精神认同', 'useless', len(useless), \",\".join(useless)])\n",
    "    \n",
    "    return pd.DataFrame(domestic_result, columns=[\"tag1\", 'tag2', '评论数', '段落'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4fdd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 国货处理\n",
    "data = pd.read_excel(cluster_input_file, sheet_name='精神认同')\n",
    "domestic = data[data['tag_2'] == '国货']\n",
    "domestic_result = clean_domestic(domestic, column_name='段落')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86c9d42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>国货</td>\n",
       "      <td>382</td>\n",
       "      <td>国货没毛病,国产无敌和30万的丰田对比30万的奔驰一个道理,支持国货结果国货一点不给力,支持...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>国货</td>\n",
       "      <td>378</td>\n",
       "      <td>祝一博和安踏都越来越好,想信特步越来越好,摩托姐祝安踏越来越好,特步加油越来越好,以后都会首...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>国货</td>\n",
       "      <td>822</td>\n",
       "      <td>国产品牌确实越做越好了,国产品牌真的是越做越好了,国产品牌越做越好了,国产品牌真的越做越好了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>国货</td>\n",
       "      <td>137</td>\n",
       "      <td>希望鸿星尔克越做越好支持国货,国牌会越来越好的,希望奇弹下一代能做的更好,最后希望国货能做得...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>精神认同</td>\n",
       "      <td>useless</td>\n",
       "      <td>1327</td>\n",
       "      <td>比同等价位品牌都好,有对比才有差距,首先盒子就非常的高级,没想到升级款全方位不如,当年县城还...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tag1     tag2   评论数                                                 段落\n",
       "0  精神认同       国货   382  国货没毛病,国产无敌和30万的丰田对比30万的奔驰一个道理,支持国货结果国货一点不给力,支持...\n",
       "1  精神认同       国货   378  祝一博和安踏都越来越好,想信特步越来越好,摩托姐祝安踏越来越好,特步加油越来越好,以后都会首...\n",
       "2  精神认同       国货   822  国产品牌确实越做越好了,国产品牌真的是越做越好了,国产品牌越做越好了,国产品牌真的越做越好了...\n",
       "3  精神认同       国货   137  希望鸿星尔克越做越好支持国货,国牌会越来越好的,希望奇弹下一代能做的更好,最后希望国货能做得...\n",
       "4  精神认同  useless  1327  比同等价位品牌都好,有对比才有差距,首先盒子就非常的高级,没想到升级款全方位不如,当年县城还..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domestic_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc86bd",
   "metadata": {},
   "source": [
    "# 二、人群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "411c2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_people(df, key_dict, column_name='段落'):\n",
    "    \n",
    "    # 自定义人群去除词\n",
    "    dropwords = ['老爹鞋','门卫大叔','麻烦','快递小哥','拿快递的小哥','办公室','公子','公主','公举','公仔','脑公','公司']\n",
    "    \n",
    "    # 建立结果字典\n",
    "    useless = []\n",
    "    people_result = defaultdict(list)\n",
    "    sentence_num = []\n",
    "    \n",
    "    def people_parser(row):\n",
    "        \"\"\"\n",
    "            row:一个类里所有语句\n",
    "            return:符合需求的一个人群类\n",
    "        \"\"\"\n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            keynum = 0                         # 标记是否是人群相关的语句\n",
    "            for k,v in key_dict.items():\n",
    "                if any(sentence.find(word) >= 0 for word in v):\n",
    "                    # 特殊情况一：其他长辈（公、叔、老公、外公、相公）\n",
    "                    if k == '其他长辈':\n",
    "                        # 有dropwords，则放入无用语料\n",
    "                        if any(sentence.find(word) >= 0 for word in dropwords):\n",
    "                            useless.append(sentence)\n",
    "                        # 有老公、外公、相公，则跳到下一个key\n",
    "                        elif any(sentence.find(word) >= 0 for word in ['老公','外公','相公']):\n",
    "                            continue\n",
    "                        # 其余放入其他长辈list\n",
    "                        else:\n",
    "                            keynum += 1\n",
    "                            people_result[k].append(sentence)\n",
    "                    \n",
    "                    # 特殊情况二：母亲（麻、爸妈、爹妈）\n",
    "                    elif k == '母亲':\n",
    "                        # 有dropwords，则放入无用语料\n",
    "                        if (sentence.find('麻烦') >= 0) & (sentence.find('麻烦') == sentence.find('麻')):\n",
    "                            useless.append(sentence)\n",
    "                        # 有爸妈、爹妈，则跳到下一个key\n",
    "                        elif any(sentence.find(word) >= 0 for word in ['爸妈','爹妈']):\n",
    "                            continue\n",
    "                        # 其余放入母亲list\n",
    "                        else:\n",
    "                            keynum += 1\n",
    "                            people_result[k].append(sentence)\n",
    "                    \n",
    "                    # 特殊情况三：父亲（爸妈、爹妈、老爹鞋）\n",
    "                    elif k == '父亲':\n",
    "                        # 有dropwords，则放入无用语料\n",
    "                        if (sentence.find('老爹鞋') >= 0) & (sentence.find('老爹鞋') == sentence.find('老爹')):\n",
    "                            useless.append(sentence)\n",
    "                        # 有爸妈、爹妈，则跳到下一个key\n",
    "                        elif any(sentence.find(word) >= 0 for word in ['爸妈','爹妈']):\n",
    "                            continue\n",
    "                        # 其余放入父亲list\n",
    "                        else:\n",
    "                            keynum += 1\n",
    "                            people_result[k].append(sentence)\n",
    "                    \n",
    "                    # 特殊情况四：婆婆（老婆、外婆）\n",
    "                    elif k == '婆婆':\n",
    "                        # 有老婆、外婆，则跳到下一个key\n",
    "                        if any(sentence.find(word) >= 0 for word in ['老婆','外婆']):\n",
    "                            continue\n",
    "                        # 其余放入婆婆list\n",
    "                        else:\n",
    "                            keynum += 1\n",
    "                            people_result[k].append(sentence)\n",
    "                    \n",
    "                    # 特殊情况五：爷爷（姥爷、老爷子）\n",
    "                    elif k == '爷爷':\n",
    "                        # 有姥爷、老爷子，则跳到下一个key\n",
    "                        if any(sentence.find(word) >= 0 for word in ['姥爷','老爷子']):\n",
    "                            continue\n",
    "                        # 其余放入爷爷list\n",
    "                        else:\n",
    "                            keynum += 1\n",
    "                            people_result[k].append(sentence)\n",
    "                    \n",
    "                    # 没有特殊情况\n",
    "                    else:\n",
    "                        keynum += 1\n",
    "                        people_result[k].append(sentence)\n",
    "                else:\n",
    "                    continue\n",
    "            if keynum == 0:\n",
    "                useless.append(sentence)\n",
    "        return people_result\n",
    "    \n",
    "    for row in df[column_name]:\n",
    "        people_result = people_parser(row)\n",
    "    \n",
    "    people_result['useless'] = useless\n",
    "    \n",
    "    final_result = []\n",
    "    for tag2, tag2_sentences in people_result.items():\n",
    "        final_result.append(['人群', tag2, len(tag2_sentences), ','.join(tag2_sentences)])\n",
    "    return pd.DataFrame(final_result, columns=['tag1', 'tag2', '评论数', '段落'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb0c58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取人群关键词列表\n",
    "tag = pd.read_excel(keyword_input_file, sheet_name='人群')\n",
    "tag_keyword = tag.groupby('tag_2').apply(lambda x: list(itertools.chain(x['keyword'].tolist())))\n",
    "people_tag_dict = dict(tag_keyword)\n",
    "# 人群处理\n",
    "data = pd.read_excel(cluster_input_file, sheet_name='人群')\n",
    "people = data[data['tag_1'] == '人群']\n",
    "people_result = clean_people(people, people_tag_dict, column_name='段落')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f89f6142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>人群</td>\n",
       "      <td>父亲</td>\n",
       "      <td>2350</td>\n",
       "      <td>老爸穿着很合适,爸穿着很合适,老爸穿得很合适,老爸穿着也很合适,老爸穿上很合适,老爸穿很合适...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>人群</td>\n",
       "      <td>母亲</td>\n",
       "      <td>1444</td>\n",
       "      <td>老妈穿着很合适,妈穿着很合适,妈穿着很合脚,老妈穿着挺合适,老妈穿上很好看,老妈穿上很合适,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>人群</td>\n",
       "      <td>对象</td>\n",
       "      <td>126</td>\n",
       "      <td>老伴穿着很合适,爱人穿着上班很合适,对象穿着挺舒服的,对象穿着很舒服,我家那位穿上很好看,对...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>人群</td>\n",
       "      <td>男朋友/老公</td>\n",
       "      <td>2535</td>\n",
       "      <td>老公穿着很合适,老公穿着很合脚,老公穿得很合适,老公穿着挺合适,老公说他穿着很合适,我老公穿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>人群</td>\n",
       "      <td>兄弟姐妹</td>\n",
       "      <td>933</td>\n",
       "      <td>我弟穿着很合适,弟穿着很合适,大哥穿上很合适,弟穿的非常合适,弟穿上很合适,弟穿很合适,弟说...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag1    tag2   评论数                                                 段落\n",
       "0   人群      父亲  2350  老爸穿着很合适,爸穿着很合适,老爸穿得很合适,老爸穿着也很合适,老爸穿上很合适,老爸穿很合适...\n",
       "1   人群      母亲  1444  老妈穿着很合适,妈穿着很合适,妈穿着很合脚,老妈穿着挺合适,老妈穿上很好看,老妈穿上很合适,...\n",
       "2   人群      对象   126  老伴穿着很合适,爱人穿着上班很合适,对象穿着挺舒服的,对象穿着很舒服,我家那位穿上很好看,对...\n",
       "3   人群  男朋友/老公  2535  老公穿着很合适,老公穿着很合脚,老公穿得很合适,老公穿着挺合适,老公说他穿着很合适,我老公穿...\n",
       "4   人群    兄弟姐妹   933  我弟穿着很合适,弟穿着很合适,大哥穿上很合适,弟穿的非常合适,弟穿上很合适,弟穿很合适,弟说..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258b07a",
   "metadata": {},
   "source": [
    "### 三、复购"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a74812",
   "metadata": {},
   "source": [
    "#### 3.1 更新换代复购"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a14013ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_update(df, column_name='段落'):\n",
    "    \n",
    "    # 自定义更新换代搜索词\n",
    "    update_search = ['代','旧款','旧版','升级','相对以前']\n",
    "    dropwords = ['希望','替代','代替','代步']  \n",
    "    update_result = []                            # 更新换代清理结果\n",
    "    useless = []                                  # 其他\n",
    "    \n",
    "    \n",
    "    def update_parser(row):\n",
    "        \"\"\"\n",
    "            row:一个类里所有语句\n",
    "            return:符合需求的一个更新换代复购类\n",
    "        \"\"\"\n",
    "        updates = []                              # 常规更新换代相关语句\n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            if any(sentence.find(word) >= 0 for word in update_search):\n",
    "                if any(sentence.find(word) >= 0 for word in dropwords):\n",
    "                    useless.append(sentence)\n",
    "                else:\n",
    "                    updates.append(sentence)\n",
    "            else:\n",
    "                useless.append(sentence)\n",
    "        \n",
    "        return ['复购', '更新换代复购', len(updates), \",\".join(updates)]\n",
    "    \n",
    "    \n",
    "    for row in df[column_name]:\n",
    "        cleaned_rows = update_parser(row)\n",
    "        update_result.append(cleaned_rows)\n",
    "    update_result.append(['复购', 'useless', len(useless), \",\".join(useless)])\n",
    "    \n",
    "    return pd.DataFrame(update_result, columns=[\"tag1\", 'tag2', '评论数', '段落'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57249907",
   "metadata": {},
   "source": [
    "#### 3.2 品牌复购"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db402521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_brand_rebuy(df, brand_list, column_name='段落'):\n",
    "    \n",
    "    # 自定义品牌复购搜索词\n",
    "    add_keys=['品牌','牌子','产品','系列']\n",
    "    special_words = brand_list + add_keys         # 所有特殊词汇\n",
    "    rebuy = ['好几','好多','一如既往','一直','不是第一次','没得说','没的说','一如继往', '向来', '习惯穿', '长期穿']\n",
    "    like = ['喜欢','值得','信得过','有保障','赞赏','认可','不愧','不错']\n",
    "    dislike = ['失望']\n",
    "    oppsite_begin_words = ['没','不','未','少']\n",
    "\n",
    "    brand_result = []                        # 更新换代清理结果\n",
    "    useless = []                             # 其他\n",
    "    \n",
    "    brand_rebuy = []                         # 常规品牌复购相关语句\n",
    "    brand_like = []                          # 品牌喜爱\n",
    "    brand_dislike = []                       # 品牌不喜爱    \n",
    "    \n",
    "    \n",
    "    def brand_parser(row):\n",
    "        \"\"\"\n",
    "            row:一个类里所有语句\n",
    "            return:符合需求的一个品牌复购类\n",
    "        \"\"\"\n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            if any(sentence.lower().find(word) >= 0 for word in special_words):\n",
    "                # 品牌复购\n",
    "                if any(sentence.find(word) >= 0 for word in rebuy) | (re.search(r'([穿买].+年)|(.+年.+[穿买])|(买.+[次双])|([^1一][双次])', sentence) != None):\n",
    "                    brand_rebuy.append(sentence)\n",
    "                # 存在品牌喜爱的词且没有否定词\n",
    "                elif any(sentence.find(word) >= 0 for word in like) & all(sentence.find(word) < 0 for word in oppsite_begin_words):\n",
    "                    brand_like.append(sentence)\n",
    "                # 品牌从没失望\n",
    "                elif any(sentence.find(word) >= 0 for word in dislike) & any(sentence.find(word) >= 0 for word in oppsite_begin_words):\n",
    "                    brand_rebuy.append(sentence)\n",
    "                else:\n",
    "                    useless.append(sentence)\n",
    "            else:\n",
    "                useless.append(sentence)\n",
    "  \n",
    "    for row in df[column_name]:\n",
    "        brand_parser(row)\n",
    "    brand_result.append(['复购', '品牌复购', len(brand_rebuy), \",\".join(brand_rebuy)])\n",
    "    brand_result.append(['精神认同', '品牌热爱', len(brand_like), \",\".join(brand_like)])  \n",
    "    brand_result.append(['复购', 'useless', len(useless), \",\".join(useless)])\n",
    "    \n",
    "    return pd.DataFrame(brand_result, columns=[\"tag1\", 'tag2', '评论数', '段落'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d648e",
   "metadata": {},
   "source": [
    "#### 3.3 常规复购"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64741ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义搜索词及jieba\n",
    "neu_begin = ['会','考虑','贵','还会','再来','值得','直得','下次','下一次','以后','以后再','以后在','还','还来','会再',\n",
    "          '值得再次','再','值得无限','有需要','有机会','有活动','还会再次','会在','还会再','还会在','会一直','打算再','打算在',\n",
    "          '会来','想','还想','要','想再','想在','应该会','还要','下次再来','可以一直','准备','下次定','下次一定','下次优惠',\n",
    "          '下次再','下次在','会持续','便于','可以','如果','会终身','会反复','打算','期待','必须','会再次','还会有','还会进行',\n",
    "          '有待','继','回来','必','回再','也要','就','说要','回头再','回头在','推荐','建议','值的']\n",
    "opp_begin = ['不','不会','不会再','不会在','不愿意','再不','在不','不会再来','再也不','不是','不敢再','不敢','绝对不','本想再',\n",
    "           '本想','不再','不在','不想','不太会','永不','再也不会','不太想再','不可能','再也没','没有']\n",
    "# will_search=['活动','准备','冲动','机会','下次','等有货']\n",
    "will_rebuy_words = ['需要','如果','有合适的','以后','合适时机','下次','会回购','值得','有活动','想法','意愿','理由','打算',\n",
    "             '合适价格','合适价','会再来','继续支持','会','准备','再购一双','需时','想','过几天','回购清单','指的',\n",
    "             '关注','之后','期待换季','期待','要万年回购','好的话','有好价','冲动','考虑','合适的话','继续买']\n",
    "no_rebuy_words = ['不要','再也不','下次不买','再没买','再不买','没有再买','以后不','以后都不','不会再','没有再']\n",
    "first_buy_words = ['第一次买','第一双','第一款','头一次','首次','第一回','第一次入手','第一次穿','第一次购买']   # 首购"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81341962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RebuyCleaner:\n",
    "    def __init__(self, first_buy_words, brand_list, rebuy_words, neu_begin_list, opp_begin_list, no_rebuy_words, will_rebuy_words):\n",
    "        self.first_buy_words = first_buy_words\n",
    "        self.brand_list = brand_list\n",
    "        self.rebuy_words = rebuy_words\n",
    "        self.neu_begin_list = neu_begin_list\n",
    "        self.opp_begin_list = opp_begin_list\n",
    "        self.no_rebuy_words = no_rebuy_words\n",
    "        self.will_rebuy_words = will_rebuy_words\n",
    "        self.useless = []\n",
    "        \n",
    "        addwords = first_buy_words + brand_list + rebuy_words + neu_begin_list + opp_begin_list + no_rebuy_words + will_rebuy_words\n",
    "        # 添加jieba自定义分词\n",
    "        for word in addwords:\n",
    "            jieba.add_word(word)    \n",
    "\n",
    "    def load_data(self, df, parse_column='段落'):\n",
    "        self.df = df\n",
    "        self.col_name = parse_column\n",
    "    \n",
    "    def parse_cluster(self):\n",
    "        rebuy_result = []\n",
    "        for row in self.df[self.col_name]:\n",
    "            self.parse_row(row)   \n",
    "            rebuy_result.append(['复购', '已经复购', len(self.have), \",\".join(self.have)])\n",
    "            rebuy_result.append(['复购', '可能复购', len(self.will), \",\".join(self.will)])\n",
    "            rebuy_result.append(['复购', '不会复购', len(self.wont), \",\".join(self.wont)])\n",
    "            rebuy_result.append(['首购', '品牌首购', len(self.first_brand), \",\".join(self.first_brand)])\n",
    "            rebuy_result.append(['首购', '其他首购', len(self.first), \",\".join(self.first)])\n",
    "        rebuy_result.append(['复购', 'useless', len(self.useless), \",\".join(self.useless)])\n",
    "        return pd.DataFrame(rebuy_result, columns=['tag1', 'tag2', '评论数', '段落'])\n",
    "    \n",
    "    def parse_row(self, row):\n",
    "        self.have = []             # 已经回购\n",
    "        self.will = []             # 会回购\n",
    "        self.wont = []             # 不会回购\n",
    "        self.first_brand = []      # 品牌首购\n",
    "        self.first = []            # 首购\n",
    "        self.check = []            # 用来检查的废语料，不相关语料\n",
    "        \n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            self.parse_sentence(sentence)\n",
    "         \n",
    "    def parse_sentence(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.word_list = [x for x in jieba.cut(self.sentence) if x not in stopwords]\n",
    "        self.flag = 0  \n",
    "        \n",
    "        if (re.search(r'购|买|来|光顾|选择|光临|次|失望|支持|关注|入|继续|下单|双|订|安排|推荐|有需要|牌|这家|店|屯|囤|收|体验|尝试|下手',\n",
    "                      self.sentence) != None) | any(self.sentence.lower().find(word) >= 0 for word in self.brand_list):        \n",
    "            self.is_first_rebuy()\n",
    "            for func in ['self.is_wont_rebuy()', 'self.is_will_rebuy()', 'self.final_rebuy_search()']:\n",
    "                    if self.flag == 0:\n",
    "                        exec(func)\n",
    "                    else:\n",
    "                        break\n",
    "        else:\n",
    "            self.useless.append(self.sentence)\n",
    "    \n",
    "    # 判断特殊品牌首购和其他首购\n",
    "    def is_first_rebuy(self):\n",
    "        for first_word in self.first_buy_words:\n",
    "            if first_word in self.word_list:\n",
    "                if first_word == '第一次穿':             # 第一次穿某个品牌则append，第一次穿去跑步等放入useless\n",
    "                    if any(self.sentence.lower().find(word) >= 0 for word in self.brand_list):\n",
    "                        self.first_brand.append(self.sentence)\n",
    "                else:    # 其他特殊首购情况\n",
    "                    index = self.word_list.index(first_word)\n",
    "                    # 不属于首购\n",
    "                    if (index > 0) & (self.word_list[index-1] in ['没','没有','不是','要','比','和']):   # 不是第一次买\n",
    "                        self.have.append(self.sentence)\n",
    "                    # 属于首购\n",
    "                    else:\n",
    "                        # 品牌首购\n",
    "                        if any(self.sentence.lower().find(word) >= 0 for word in self.brand_list):\n",
    "                            self.first_brand.append(self.sentence)\n",
    "                        # 其他首购\n",
    "                        else:\n",
    "                            self.first.append(self.sentence)\n",
    "                break\n",
    "    \n",
    "    # 判断特殊不会复购，会存在可能复购的情况\n",
    "    def is_wont_rebuy(self):\n",
    "        if any(self.sentence.find(word) >= 0 for word in self.no_rebuy_words):\n",
    "#             if self.sentence.find('还') >= 0:\n",
    "#                 self.will.append(self.sentence)\n",
    "#             else:\n",
    "#                 self.wont.append(self.sentence)\n",
    "            self.wont.append(self.sentence)\n",
    "            self.flag = 1\n",
    "    \n",
    "    # 判断特殊可能复购，会存在不会复购等情况\n",
    "    def is_will_rebuy(self):\n",
    "        is_opposite = 0\n",
    "        if any(self.sentence.find(word) >= 0 for word in self.will_rebuy_words):\n",
    "            # 出现已经复购\n",
    "            if any(self.sentence.find(word) >= 0 for word in ['都会回购']):    # 有需要都会回购，说明已复购\n",
    "                self.have.append(self.sentence)\n",
    "                self.flag = 1\n",
    "            # 一定是可能复购\n",
    "            elif re.search(r'还|如果|的话', self.sentence) != None:                      #还会来\n",
    "#             self.sentence.find('还') >= 0:                       \n",
    "                self.will.append(self.sentence)\n",
    "                self.flag = 1\n",
    "            # 出现无用语句\n",
    "            elif self.sentence.find('不会后悔') >= 0:\n",
    "                self.useless.append(self.sentence)\n",
    "                self.flag = 1\n",
    "            # 在不满足上述条件的情况下判断是否属于不会复购\n",
    "            else: \n",
    "#                 for word in self.word_list:\n",
    "                if (sum([1 if word in self.opp_begin_list else 0 for word in self.word_list ]) > 0) | any(self.sentence.find(word) >= 0 for word in self.no_rebuy_words):\n",
    "                    if '不会失望' not in self.sentence:                 # 不会失望\n",
    "                        self.wont.append(self.sentence)\n",
    "                        self.flag = 1\n",
    "                    is_opposite = 1  \n",
    "                if is_opposite == 0:\n",
    "                    self.will.append(self.sentence)\n",
    "                    self.flag = 1\n",
    "                                        \n",
    "    # 常规情况整体搜索\n",
    "    def final_rebuy_search(self):\n",
    "        for index, word in enumerate(self.word_list):\n",
    "            if word in self.rebuy_words:\n",
    "                self.flag = 1\n",
    "                if word in ['买了一双','买一双','一双', '买了一件','买了双']:\n",
    "                    if any(self.sentence.find(word) >= 0 for word in ['又','也','回购','各','再','再买', '也是','还','和','次','之前']):\n",
    "                        self.have.append(self.sentence)\n",
    "                    else:\n",
    "                        self.useless.append(self.sentence)\n",
    "                # 根据关键词的前一个词判断属于可能复购/不会复购/已经复购\n",
    "                elif (self.word_list[index-1] in self.neu_begin_list) & (index > 0):\n",
    "                    self.will.append(self.sentence)\n",
    "                elif (self.word_list[index-1] in self.opp_begin_list) & (index > 0):\n",
    "                    if '不会失望' not in self.sentence:\n",
    "                        self.wont.append(self.sentence)\n",
    "                else:\n",
    "                    self.have.append(self.sentence)\n",
    "                break\n",
    "        if re.search(r'又[帮|给].+买', self.sentence) != None:\n",
    "            self.have.append(self.sentence)\n",
    "            self.flag = 1\n",
    "        elif (re.search(r'一直.+[穿|买]', self.sentence) != None) & any(self.sentence.find(word) >= 0 for word in (self.brand_list + ['品牌','牌子'])):\n",
    "            self.have.append(self.sentence)\n",
    "            self.flag = 1\n",
    "        else:\n",
    "            pass\n",
    "        if (self.flag == 0) & (self.sentence not in (self.first + self.first_brand)):\n",
    "            self.useless.append(self.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2b54fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(cluster_input_file, sheet_name='首购复购')\n",
    "# 更新换代复购处理\n",
    "update = data[data['tag_2'] == '产品']\n",
    "update_result = clean_update(update, column_name='段落')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40dc2e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>复购</td>\n",
       "      <td>更新换代复购</td>\n",
       "      <td>195</td>\n",
       "      <td>没有上次14代的那个设计合理,感觉这一代不如上一代用料足啊,感觉没有上一代用心了,这一代实物...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>复购</td>\n",
       "      <td>useless</td>\n",
       "      <td>674</td>\n",
       "      <td>这个880替代真的不错,没有之前的那种高科技盒子,但相比alphafly那种后掌没有橡胶还是...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag1     tag2  评论数                                                 段落\n",
       "0   复购   更新换代复购  195  没有上次14代的那个设计合理,感觉这一代不如上一代用料足啊,感觉没有上一代用心了,这一代实物...\n",
       "1   复购  useless  674  这个880替代真的不错,没有之前的那种高科技盒子,但相比alphafly那种后掌没有橡胶还是..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec0f470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取品牌列表\n",
    "data_brand = pd.read_excel(keyword_input_file, sheet_name='品牌')\n",
    "brand = data_brand['brand'].astype(str).str.lower().tolist()\n",
    "# 处理品牌复购\n",
    "brand_data = data[data['tag_2'] == '品牌']\n",
    "brand_result = clean_brand_rebuy(brand_data, brand_list=brand, column_name='段落',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0f15d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>复购</td>\n",
       "      <td>品牌复购</td>\n",
       "      <td>1010</td>\n",
       "      <td>迪卡侬的鞋没失望过,迪卡侬的鞋没的说,迪卡侬没失望过,迪卡侬的鞋子没的说,乔丹的鞋子没让我失...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>复购</td>\n",
       "      <td>品牌热爱</td>\n",
       "      <td>1300</td>\n",
       "      <td>就喜欢斯凯奇的鞋底,太喜欢买的斯凯奇旅游鞋,就喜欢斯凯奇家的鞋,就喜欢斯凯奇的鞋子,就喜欢斯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>复购</td>\n",
       "      <td>useless</td>\n",
       "      <td>2950</td>\n",
       "      <td>对迪卡侬这个鞋失望极了,斯凯奇的鞋子没问题,安踏的鞋子原来越失望了,斯凯奇的鞋真心好穿,斯凯...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag1     tag2   评论数                                                 段落\n",
       "0   复购     品牌复购  1010  迪卡侬的鞋没失望过,迪卡侬的鞋没的说,迪卡侬没失望过,迪卡侬的鞋子没的说,乔丹的鞋子没让我失...\n",
       "1   复购     品牌热爱  1300  就喜欢斯凯奇的鞋底,太喜欢买的斯凯奇旅游鞋,就喜欢斯凯奇家的鞋,就喜欢斯凯奇的鞋子,就喜欢斯...\n",
       "2   复购  useless  2950  对迪卡侬这个鞋失望极了,斯凯奇的鞋子没问题,安踏的鞋子原来越失望了,斯凯奇的鞋真心好穿,斯凯..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73f3d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不会复购，首购，可能回购，已经回购\n",
    "# 读取复购关键词\n",
    "rebuy_keywords = pd.read_excel(keyword_input_file, sheet_name='复购')\n",
    "rebuy_keywords = rebuy_keywords['keyword'].tolist()\n",
    "brand.append('碳板')\n",
    "\n",
    "rebuy_clusters = data[data['tag_2'] == '复购']\n",
    "clean_rebuy = RebuyCleaner(first_buy_words, brand, rebuy_keywords, neu_begin, opp_begin, no_rebuy_words, will_rebuy_words)\n",
    "clean_rebuy.load_data(rebuy_clusters, parse_column='段落')\n",
    "result = clean_rebuy.parse_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0eb9e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存所有复购类的清理结果\n",
    "rebuy_result = pd.concat([update_result, brand_result, result])\n",
    "rebuy_result = rebuy_result.query(\"评论数 > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26a16e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>复购</td>\n",
       "      <td>更新换代复购</td>\n",
       "      <td>195</td>\n",
       "      <td>没有上次14代的那个设计合理,感觉这一代不如上一代用料足啊,感觉没有上一代用心了,这一代实物...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>复购</td>\n",
       "      <td>useless</td>\n",
       "      <td>674</td>\n",
       "      <td>这个880替代真的不错,没有之前的那种高科技盒子,但相比alphafly那种后掌没有橡胶还是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>复购</td>\n",
       "      <td>品牌复购</td>\n",
       "      <td>1010</td>\n",
       "      <td>迪卡侬的鞋没失望过,迪卡侬的鞋没的说,迪卡侬没失望过,迪卡侬的鞋子没的说,乔丹的鞋子没让我失...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>复购</td>\n",
       "      <td>品牌热爱</td>\n",
       "      <td>1300</td>\n",
       "      <td>就喜欢斯凯奇的鞋底,太喜欢买的斯凯奇旅游鞋,就喜欢斯凯奇家的鞋,就喜欢斯凯奇的鞋子,就喜欢斯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>复购</td>\n",
       "      <td>useless</td>\n",
       "      <td>2950</td>\n",
       "      <td>对迪卡侬这个鞋失望极了,斯凯奇的鞋子没问题,安踏的鞋子原来越失望了,斯凯奇的鞋真心好穿,斯凯...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag1     tag2   评论数                                                 段落\n",
       "0   复购   更新换代复购   195  没有上次14代的那个设计合理,感觉这一代不如上一代用料足啊,感觉没有上一代用心了,这一代实物...\n",
       "1   复购  useless   674  这个880替代真的不错,没有之前的那种高科技盒子,但相比alphafly那种后掌没有橡胶还是...\n",
       "0   复购     品牌复购  1010  迪卡侬的鞋没失望过,迪卡侬的鞋没的说,迪卡侬没失望过,迪卡侬的鞋子没的说,乔丹的鞋子没让我失...\n",
       "1   复购     品牌热爱  1300  就喜欢斯凯奇的鞋底,太喜欢买的斯凯奇旅游鞋,就喜欢斯凯奇家的鞋,就喜欢斯凯奇的鞋子,就喜欢斯...\n",
       "2   复购  useless  2950  对迪卡侬这个鞋失望极了,斯凯奇的鞋子没问题,安踏的鞋子原来越失望了,斯凯奇的鞋真心好穿,斯凯..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebuy_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8ca48b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuy_result.to_excel(\"复购全部.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f9875",
   "metadata": {},
   "source": [
    "### 四、场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7c11c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_situation(df, key_dict, column_name='段落'):\n",
    "    \n",
    "    # 自定义否定词\n",
    "    none = ['不合适','不适合','不能','不太','无法','没法','还是算了','就算了','不推荐','不能作为','不能拿来','免了','别说了','别想了','差',\n",
    "      '更别说','千万别','别说','不舒服','慎重','疼','痛','累','烧','磨','硌','不是太好','重','硬','受伤','不好','不太能','挤','沉','顿',\n",
    "      '自求多福','别考虑','三思','捂','顶脚','崴','压脚','夹脚','臭','不透气','难受','不舒适','勒','不是很合适','不太行','闷','不行',\n",
    "      '更不要说','不太好','不要','不跟脚','磕','别扭','咯吱','嘎吱','滑','不是很好','不建议','不适宜','不够','不足','不是很舒服','不稳定',\n",
    "      '不怎么舒服','泡','失望','问题','不用来','不指望','不是','不可以','不大行','不大舒服','不方便','有点影响','不是很方便','不好控制','没有提升',\n",
    "      '不适宜']\n",
    "    neg_sentiment = ['疼','痛','累','烧','磨','硌','重','硬','受伤','挤','沉','顿','捂','顶脚','崴','压脚','夹脚','臭','难受','勒','闷',\n",
    "               '磕','别扭','咯吱','嘎吱','滑','泡','失望','问题','差']  # 前面出现否定词会变成正面的\n",
    "    \n",
    "    # 建立结果字典\n",
    "    useless = []                                   # 其他\n",
    "    situation_positive = defaultdict(list)         # 正面\n",
    "    situation_negative = defaultdict(list)         # 负面\n",
    "    sentence_num_pos = []\n",
    "    sentence_num_neg = []\n",
    "    \n",
    "    def refresh_keyword(k, keynum, sentence, key, dropword, searchword):\n",
    "        flag = 0\n",
    "        if k == key:\n",
    "            if (sentence.find(dropword) >= 0) & (sentence.find(searchword) == sentence.find(dropword)): # 如果是街舞里搜到街等于没有搜到\n",
    "                keynum -= 1\n",
    "                flag += 1\n",
    "        return flag\n",
    "\n",
    "    def divide_sentiment(k, forsearch_sentence):\n",
    "        # 判断否定\n",
    "        if any(forsearch_sentence.find(word) >= 0 for word in none):\n",
    "            # 特殊情况1：不太、不是\n",
    "            if any(forsearch_sentence.find(word) >= 0 for word in ['不太','不是']):\n",
    "                if any(forsearch_sentence.find(word) >= 0 for word in ['适合','合适','建议','推荐','舒服','不太行','不太能','不太好','不是专业']):\n",
    "                    situation_negative[k].append(forsearch_sentence)\n",
    "                else:\n",
    "                    situation_positive[k].append(forsearch_sentence)\n",
    "                            \n",
    "            # 特殊情况2：差\n",
    "            elif any(forsearch_sentence.find(word) >= 0 for word in ['出差','差不多']): # 没有反面的意思\n",
    "                situation_positive[k].append(forsearch_sentence)\n",
    "            \n",
    "            # 特殊情况3：负面情感词\n",
    "            elif any(forsearch_sentence.find(word) >= 0 for word in neg_sentiment):  # neg_sentiment里的词加上前置否定又变成了正面的\n",
    "                # 3-1 出现否定词，则 positive append\n",
    "                if any(forsearch_sentence.find(word) >= 0 for word in ['不','没','无','防','耐']):\n",
    "                    situation_positive[k].append(forsearch_sentence)\n",
    "                # 3-2 重\n",
    "                elif any(forsearch_sentence.find(word) >= 0 for word in ['重新','重力','体重','重量','负重','重庆']):\n",
    "                    situation_positive[k].append(forsearch_sentence)\n",
    "                # 3-3 滑\n",
    "                elif any(forsearch_sentence.find(word) >= 0 for word in ['滑板','滑雪']):\n",
    "                    situation_positive[k].append(forsearch_sentence)\n",
    "                # 3-4 其他均 negative append\n",
    "                else:\n",
    "                    situation_negative[k].append(forsearch_sentence)\n",
    "            # 其余搜索到none的情况\n",
    "            else:\n",
    "                situation_negative[k].append(forsearch_sentence)\n",
    "        # 没有否定\n",
    "        else:\n",
    "            situation_positive[k].append(forsearch_sentence)\n",
    "    \n",
    "    \n",
    "    def situation_parser(row):\n",
    "        \"\"\"\n",
    "            row:一个类里所有语句\n",
    "            return:符合需求的一个适用场景类并区分正负\n",
    "        \"\"\"\n",
    "        sentences_list = row.split(',')\n",
    "        for sentence in sentences_list:\n",
    "            keynum = 0    # 一句话里tag2的个数\n",
    "            # 根据关键词判断\n",
    "            for k,v in key_dict.items():\n",
    "                if any(sentence.find(word) >= 0 for word in v):\n",
    "                    \n",
    "                    keynum += 1\n",
    "                    \n",
    "                    # 特殊情况一：搭配\n",
    "                    if k == '搭配':\n",
    "                        if sentence.find('不好搭配') >= 0:\n",
    "                            situation_negative[k].append(sentence)\n",
    "                        elif (sentence.find('难搭') >= 0) & (sentence.find('不') == -1):\n",
    "                            situation_negative[k].append(sentence)\n",
    "                        else:\n",
    "                            situation_positive[k].append(sentence)\n",
    "                    \n",
    "                    # 特殊情况二：日常休闲\n",
    "                    elif k == '日常休闲':\n",
    "                        flag = refresh_keyword(k, keynum, sentence, '日常休闲', '街舞', '街')\n",
    "                        if flag == 1:\n",
    "                            continue\n",
    "                        if any(sentence.find(word) >= 0 for word in ['不太能','不合适','不是很合适','不适合','不是很适合']):\n",
    "                            situation_negative[k].append(sentence)\n",
    "                        else:\n",
    "                            situation_positive[k].append(sentence)\n",
    "                    \n",
    "                    # 特殊情况三：在跑步中搜到跑步鞋，则跳过\n",
    "                    elif (k == '跑步') & (sentence.find('跑步鞋') >= 0) & (sentence.find('跑步') == sentence.find('跑步鞋')):\n",
    "                        continue\n",
    "                    \n",
    "                    # 其他情况\n",
    "                    else:\n",
    "                        # 运动鞋\n",
    "                        flag_1 = refresh_keyword(k, keynum, sentence, '其他运动', '运动鞋', '运动')\n",
    "                        # 健步如飞\n",
    "                        flag_2 = refresh_keyword(k, keynum, sentence, '健步', '健步如飞', '健步')\n",
    "                         # 体育用品\n",
    "                        flag_3 = refresh_keyword(k, keynum, sentence, '体育课及考试', '体育用品', '体育')\n",
    "                        # 其余常规\n",
    "                        if max(flag_1,flag_2,flag_3) == 0:\n",
    "                            divide_sentiment(k, sentence)\n",
    "                else: # 没有搜索到关键词\n",
    "                    continue             \n",
    "            \n",
    "            # 根据正则表达式判断\n",
    "            # 1-跳X舞\n",
    "            if (re.search(r'跳.+舞', sentence) != None) & (sentence.find('跳广场舞') == -1):\n",
    "                keynum += 1\n",
    "                divide_sentiment('跳舞', sentence)\n",
    "            # 2-平时穿\n",
    "            if any(sentence.find(word) >= 0 for word in ['平时穿','平常穿']):\n",
    "                keynum += 1\n",
    "                if any(sentence.find(word) >= 0 for word in ['不太能','不合适','不是很合适','不适合','不是很适合']):\n",
    "                    situation_negative['日常休闲'].append(sentence)\n",
    "                else:\n",
    "                    situation_positive['日常休闲'].append(sentence)\n",
    "            \n",
    "            if keynum == 0:      \n",
    "                useless.append(sentence)\n",
    "        return situation_positive, situation_negative\n",
    "    \n",
    "    # 逐行清理\n",
    "    for row in df[column_name]:\n",
    "        situation_pos_result, situation_neg_result = situation_parser(row)\n",
    "    \n",
    "    # 保存清理完的结果\n",
    "    final_result = []\n",
    "    for tag2, tag2_sentences in situation_pos_result.items():\n",
    "        final_result.append(('适用场景', tag2, 'positive', len(tag2_sentences), \",\".join(tag2_sentences)))\n",
    "    \n",
    "    for tag2, tag2_sentences in situation_neg_result.items():\n",
    "        final_result.append(('适用场景', tag2, 'negative', len(tag2_sentences), \",\".join(tag2_sentences)))\n",
    "    \n",
    "    final_result.append(('适用场景', \"useless\", None, len(useless), \",\".join(useless)))\n",
    "    \n",
    "    final_result = pd.DataFrame(final_result, columns=['tag1', 'tag2', '情感倾向', '评论数', '段落'])\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ff0d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(cluster_input_file, sheet_name='适用场景')\n",
    "data = data[data['tag_1'] == '适用场景']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e3fce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取适用场景列表\n",
    "tag = pd.read_excel(keyword_input_file, sheet_name='场景')\n",
    "tag_keyword=tag.groupby('tag_2').apply(lambda x: list(itertools.chain(x['keyword'].tolist())))\n",
    "tag_keyword = dict(tag_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f90344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_result = clean_situation(data, tag_keyword, column_name='段落')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "087f0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_result.to_excel(\"场景清理.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4b8e68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>情感倾向</th>\n",
       "      <th>评论数</th>\n",
       "      <th>段落</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>适用场景</td>\n",
       "      <td>跑步</td>\n",
       "      <td>正面</td>\n",
       "      <td>27881</td>\n",
       "      <td>跑步起来脚感很棒,跑起步来脚感很好,跑步脚感很棒,跑起来脚感超棒,跑步脚感非常棒,跑起来脚感...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>适用场景</td>\n",
       "      <td>走路</td>\n",
       "      <td>正面</td>\n",
       "      <td>6575</td>\n",
       "      <td>走路脚感也很棒,走路脚感很棒,走路的脚感也非常棒,走路跑步脚感不错,走路脚感也很好,跑步走路...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>适用场景</td>\n",
       "      <td>跳绳</td>\n",
       "      <td>正面</td>\n",
       "      <td>1059</td>\n",
       "      <td>跳绳跑步轻盈感挺好,跑步跳绳都特别轻便,跑步跳绳都很轻便,跑步跳绳刚好而且很轻,跑步走路跳绳...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>适用场景</td>\n",
       "      <td>其他运动</td>\n",
       "      <td>正面</td>\n",
       "      <td>3047</td>\n",
       "      <td>运动起来很跟脚,跑步运动很轻便,运动起来很舒适,跑步锻炼时感觉很有力,跑步的时候略微的弹跳力...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>适用场景</td>\n",
       "      <td>健身</td>\n",
       "      <td>正面</td>\n",
       "      <td>674</td>\n",
       "      <td>跑步的时候略微的弹跳力很适合健身运动,健身休闲百搭,感觉挺适合跑步健身的,跑步健身挺舒服的,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tag1  tag2 情感倾向    评论数                                                 段落\n",
       "0  适用场景    跑步   正面  27881  跑步起来脚感很棒,跑起步来脚感很好,跑步脚感很棒,跑起来脚感超棒,跑步脚感非常棒,跑起来脚感...\n",
       "1  适用场景    走路   正面   6575  走路脚感也很棒,走路脚感很棒,走路的脚感也非常棒,走路跑步脚感不错,走路脚感也很好,跑步走路...\n",
       "2  适用场景    跳绳   正面   1059  跳绳跑步轻盈感挺好,跑步跳绳都特别轻便,跑步跳绳都很轻便,跑步跳绳刚好而且很轻,跑步走路跳绳...\n",
       "3  适用场景  其他运动   正面   3047  运动起来很跟脚,跑步运动很轻便,运动起来很舒适,跑步锻炼时感觉很有力,跑步的时候略微的弹跳力...\n",
       "4  适用场景    健身   正面    674  跑步的时候略微的弹跳力很适合健身运动,健身休闲百搭,感觉挺适合跑步健身的,跑步健身挺舒服的,..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situation_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a618cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有类\n",
    "all_result = pd.concat([endorsement_result, domestic_result, people_result, rebuy_result, situation_result])\n",
    "all_result['is_useless'] = all_result['tag2'].str.contains('useless')\n",
    "all_result = all_result[all_result['is_useless'] == False]\n",
    "all_result = all_result.drop(['is_useless'], axis=1)\n",
    "all_result.to_csv('step_data/part4_clean_special_clusters_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
