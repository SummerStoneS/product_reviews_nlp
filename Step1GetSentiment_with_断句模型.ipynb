{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fb60ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\rshe11\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.607 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from snownlp import sentiment, SnowNLP\n",
    "import re\n",
    "import nlpir\n",
    "from langconv import Converter\n",
    "import glob\n",
    "from utils import get_source_comments_data, read_list, sepSentences, GetWordDict, filter_cutting_sentence\n",
    "from my_sentiment import cal_score, keyword_tag\n",
    "from my_sentence_cut import cut_sentences\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90025989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snownlp给每条评论计算情感分数\n",
    "def classify_sentiment(unique_paragraphs):\n",
    "    positive_paragraphs = []\n",
    "    negative_paragraphs = []\n",
    "    neutral_paragraphs = []\n",
    "    paragraph_sentiment_list = []\n",
    "    for par in tqdm.tqdm(unique_paragraphs):\n",
    "        s_par = SnowNLP(par)\n",
    "        if s_par.sentiments > 0.6:\n",
    "            positive_paragraphs.append(par)\n",
    "            s = 'positive'\n",
    "        elif s_par.sentiments < 0.3:\n",
    "            negative_paragraphs.append(par)\n",
    "            s = 'negative'\n",
    "        else:\n",
    "            neutral_paragraphs.append(par)\n",
    "            s = 'neutral'\n",
    "        paragraph_sentiment_list.append((par, s_par.sentiments, s))\n",
    "    return positive_paragraphs, negative_paragraphs, neutral_paragraphs, paragraph_sentiment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70db3dc",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf467b80",
   "metadata": {},
   "source": [
    "#### 读取原始评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f568a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始评价数： 911098\n",
      "原始不为空评论数: 911057\n"
     ]
    }
   ],
   "source": [
    "dataset = get_source_comments_data(is_first_time=False)\n",
    "print(\"原始评价数：\", len(dataset))\n",
    "comments_list = dataset[\"评论内容\"].dropna().tolist()\n",
    "print(\"原始不为空评论数:\", len(comments_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cafd9",
   "metadata": {},
   "source": [
    "#### Step1 给每条评论断句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bef1917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "断句后的段落数: 3162954\n",
      "断句后的非重复段落数： 1189184\n"
     ]
    }
   ],
   "source": [
    "# 简单断句--标点符号转折语断句\n",
    "sentences = sepSentences(comments_list)\n",
    "print(\"断句后的段落数:\", len(sentences))\n",
    "unique_sentences = pd.Series(sentences).drop_duplicates().tolist()\n",
    "print(\"断句后的非重复段落数：\", len(unique_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3813d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 用断句模型断句\n",
    "# 1.挑出来一部分符合条件的断句\n",
    "sentence_to_cut, remain = filter_cutting_sentence(unique_sentences)\n",
    "pd.Series(remain).to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='remain')\n",
    "pd.Series(sentence_to_cut).to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='sentence_to_cut')\n",
    "# 2.调用断句模型断句\n",
    "model_path = \"断句/nike_comment-master/checkpoint/wwm_conv1d-all-000_modeling_comments.pkl\"\n",
    "bert_name = 'hfl/chinese-bert-wwm-ext'\n",
    "cut_result = cut_sentences(sentence_to_cut, model_path, bert_name)\n",
    "cut_result.to_hdf(r\"step_data\\sentence_cut\\sentence_cut.hdf\", key='cut_result')\n",
    "sentence_to_cut = itertools.chain(*cut_result['cut_result'].tolist())\n",
    "sentence_to_cut = [sentence for sentence in sentence_to_cut if (re.search('[\\u4e00-\\u9fa5]', sentence) != None) and (len(sentence) > 1)]\n",
    "remain.extend(sentence_to_cut)         \n",
    "\n",
    "# 首次运行并保存结果 时间很长\n",
    "positive, negative, neutral, paragraph_sentiment_list = classify_sentiment(remain)\n",
    "pd.DataFrame(paragraph_sentiment_list, columns=[\"text\",'senti_score','sentiment']).to_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83619f5d",
   "metadata": {},
   "source": [
    "#### Step2 snownlp情感判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e004a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive, negative, neutral, paragraph_sentiment_list = classify_sentiment(unique_sentences)\n",
    "# pd.DataFrame(paragraph_sentiment_list, columns=[\"text\",'senti_score','sentiment']).to_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab40792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取结果\n",
    "paragraph_sentiment_list = pd.read_hdf(\"step_data/snownlp_sentiments_result.hdf\", \"main\") # snownlp的情感分析结果\n",
    "positive = paragraph_sentiment_list.query(\"sentiment=='positive'\")\n",
    "negative = paragraph_sentiment_list.query(\"sentiment=='negative'\")\n",
    "neutral = paragraph_sentiment_list.query(\"sentiment=='neutral'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df05347",
   "metadata": {},
   "source": [
    "#### Step3 修正情感判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己设计的情感分析算法给语句分正负面中性\n",
    "neg_result, neg_nokeywords = cal_score(negative[\"text\"].to_list(), orisen_tag=\"负面\")\n",
    "print(\"中性\", len(neg_result))\n",
    "pos_result, pos_nokeywords = cal_score(positive[\"text\"].to_list(), orisen_tag=\"正面\")\n",
    "print('正面', len(pos_result))\n",
    "neu_result, neu_nokeywords = cal_score(neutral[\"text\"].to_list(), orisen_tag=\"中性\")\n",
    "print('负面', len(neu_result))\n",
    "result = pd.concat([neg_result, neu_result, pos_result, neg_nokeywords, pos_nokeywords, pos_nokeywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26970e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把复购，人群，场景，精神认同的先筛选出来不进入到聚类了\n",
    "psbe = result[result['tag_1'].isin(['复购', '人群', '适用场景', '精神认同'])]\n",
    "psbe.to_excel(r\"model_result/sentiment/rebuy_people_situation_endorsement.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1524637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_result = result[~result['tag_1'].isin(['复购', '人群', '适用场景', '精神认同'])]\n",
    "my_sentiment_result = normal_result[normal_result[\"keyword\"].notnull()]  \n",
    "nokeyword_result = normal_result[normal_result[\"keyword\"].isnull()]      # 没有keyword的句子，包括只有comment的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bb2f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\AppData\\Local\\Temp/ipykernel_11056/4216519342.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nokeyword_result['sentiment'] = np.where((nokeyword_result['sentiment'] == '正面') & (nokeyword_result['orisen_tag'].isin(['负面','中性'])), '中性', nokeyword_result['sentiment'])\n"
     ]
    }
   ],
   "source": [
    "# # 检查自主开发的情感分析和snownlp结果不一样的\n",
    "# mismatch=nokeyword_result[(nokeyword_result['sentiment'] != nokeyword_result['orisen_tag']) & nokeyword_result['sentiment'].notnull()]\n",
    "# mismatch.to_excel(\"check/mismatch_sentiment_v3.xlsx\", index=False)\n",
    "\n",
    "# 只有comment的句子，my sentiment是中性和负面的还用自己的，正面的用snownlp的,但是\n",
    "# nokeyword_result['sentiment'] = np.where((nokeyword_result['sentiment'] == '正面') & (nokeyword_result['orisen_tag'].isin(['负面','中性'])), '中性', nokeyword_result['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b203a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rshe11\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2703: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['sentence', 'segmentation', 'orisen_tag', 'sentiment', 'keyword',\n",
      "       'tag_1', 'tag_2'],\n",
      "      dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "# 保存结果\n",
    "my_sentiment_result.to_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"keyword\")  # 有分数的为有keyword或者no_keyword里有情感词的即可以计算分数的\n",
    "my_sentiment_result.to_csv(r\"model_result/sentiment/comments_sentiment_keywords.csv\", index=False, encoding='utf-8')\n",
    "nokeyword_result.to_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"no_keyword\")\n",
    "nokeyword_result.to_csv(r\"model_result/sentiment/comments_sentiment_no_keywords.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd0a46",
   "metadata": {},
   "source": [
    "#### Step4 每类情感生成一个文件，为后续聚类做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "146a13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_useless_neutral_sentences(my_sentiment_result):\n",
    "    \"\"\"\n",
    "        如果一句话已经有正面或者负面，那就没必要再在中性里出现了\n",
    "    \"\"\"\n",
    "    sentence_sentiment_num = my_sentiment_result.groupby(['sentence','sentiment'])['keyword'].count()\n",
    "    sentence_sentiment_num = sentence_sentiment_num.unstack()\n",
    "    return set(sentence_sentiment_num[((sentence_sentiment_num['正面'] > 0)|(sentence_sentiment_num['负面'] > 0)) & (sentence_sentiment_num['中性'] > 0)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7723f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面的句子数量：212047\n",
      "负面的句子数量：104587\n",
      "中性的句子数量：80929\n",
      "no keyword\n",
      "正面 86210\n",
      "负面 111211\n",
      "中性 86566\n"
     ]
    }
   ],
   "source": [
    "my_sentiment_result = pd.read_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"keyword\")\n",
    "sentiment_en = {'正面': 'positive', '负面': 'negative', '中性':'neutral'}\n",
    "for sentiment_type in ['正面', '负面', '中性']: \n",
    "    sentiments_data = my_sentiment_result[my_sentiment_result[\"sentiment\"] == sentiment_type]\n",
    "    sentiments_data = sentiments_data[\"sentence\"].unique().tolist()\n",
    "    print(f\"{sentiment_type}的句子数量：{len(sentiments_data)}\")\n",
    "    if sentiment_type == '中性':\n",
    "        remove_sentences = filter_useless_neutral_sentences(my_sentiment_result)\n",
    "        sentiments_data = [sentence for sentence in set(sentiments_data) if sentence not in remove_sentences]\n",
    "    simplified_sentences = [sentence for sentence in sentiments_data if ((len(sentence) <= 50) and (len(sentence) >= 2))]\n",
    "    with open(f\"model_result/sentiment/{sentiment_en[sentiment_type]}_sentence_list.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(simplified_sentences))\n",
    "        \n",
    "nokeyword_result = pd.read_hdf(\"model_result/sentiment/comments_sentiment_result.hdf\", \"no_keyword\")\n",
    "print(\"no keyword\")\n",
    "for sentiment_type in ['正面', '负面', '中性']:\n",
    "    no_keyword_data = nokeyword_result.query(\"sentiment == @sentiment_type\")\n",
    "    others = no_keyword_data[\"sentence\"].unique().tolist()\n",
    "    print(sentiment_type, len(others))\n",
    "    simplified_sentences = [sentence for sentence in others if ((len(sentence) <= 50) and (len(sentence) >= 2))]\n",
    "    with open(f\"model_result/sentiment/no_keyword_{sentiment_en[sentiment_type]}_sentence_list.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(simplified_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852901d",
   "metadata": {},
   "source": [
    "#####################         end         ##########################\n",
    "再往下不用运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec1c69ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(681968, 8)\n",
      "(23592, 8)\n"
     ]
    }
   ],
   "source": [
    "# 以下不用运行\n",
    "print(my_sentiment_result[my_sentiment_result['sentence'].str.len() <= 20].shape)\n",
    "print(my_sentiment_result[my_sentiment_result['sentence'].str.len() > 20].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1e7b0",
   "metadata": {},
   "source": [
    "my_sentiment_result[my_sentiment_result['sentence'].str.len() > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21345e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in long_sentence.index.to_list():\n",
    "        f.write(str(sentence))\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_sentiment_num2[(sentence_sentiment_num2['正面'] > 0) & (sentence_sentiment_num2['中性'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c760bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"from_Doris/comments_sentiment_result_with_tag_ABCDEF.hdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
